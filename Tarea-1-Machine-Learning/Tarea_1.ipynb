{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ecddaed7eb5446599eb9374acef88d65",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> # **Tarea 1: An√°lisis Exploratorio de Datos - Pok√©mon**\n",
    "> #### **AST332-1 - 2025S2**\n",
    ">\n",
    "> Luciano Laroze, Cristi√°n N√∫√±ez.\n",
    ">\n",
    "> *Universidad T√©cnica Federico Santa Mar√≠a, San Joaqu√≠n, Chile.*\n",
    ">\n",
    "> Nuestro repositorio de GitHub puede encontrarse en los siguientes usuarios\n",
    ">\n",
    "> C. N√∫√±ez [@ccanunez](https://github.com/ccanunez/Machine-Learning-2025-2.git, 'Reposotirio de C. N√∫√±ez')\n",
    ">\n",
    "> L. Laroze [@LuziVGC](https://github.com/LuziVGC, 'Reposotirio de L. Laroze')\n",
    ">\n",
    "> **Objetivo** : familiarizarse con el dataset, identificar patrones o anomal√≠as y obtener una comprensi√≥n general de los datos para futuras aplicaciones de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b95987a17b5a45098fb77888fd5b4ced",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> # **Descripci√≥n y carga de datos**\n",
    ">\n",
    "> Se eligi√≥ el set de datos correspondiente al arreglo CSV de Pok√©mon.\n",
    ">\n",
    "> A continuaci√≥n se cargan librer√≠as para el uso (`numpy`) y manejo (`pandas`) de datos, as√≠ como la visualizaci√≥n (`matplotlib` y `seaborn`) de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global style settings for better quality plots\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 300,         # DPI for high-quality export\n",
    "    \"savefig.dpi\": 300,        # DPI when saving figures\n",
    "    \"figure.figsize\": (6, 4),  # A good balance: not too big, not too small\n",
    "    \"axes.labelsize\": 12,      # Axis label font size\n",
    "    \"axes.titlesize\": 14,      # Title font size\n",
    "    \"xtick.labelsize\": 10,     # X-tick label size\n",
    "    \"ytick.labelsize\": 10,     # Y-tick label size\n",
    "    \"lines.linewidth\": 1.5,    # Slightly thicker lines\n",
    "    \"lines.markersize\": 6,     # Marker size\n",
    "    \"legend.fontsize\": 10,     # Legend font size\n",
    "    \"grid.alpha\": 0.3,         # Softer grid\n",
    "    \"axes.grid\": True,         # Grid on by default\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos en formato DataFrame\n",
    "data = pd.read_csv('pokemon.csv')\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de las primeras 5 l√≠neas del DataFrame\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripci√≥n estad√≠stica del set de datos\n",
    "df = data.copy()\n",
    "describe = df.describe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "40db2221bb7f4607bb614fe19d7ee869",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "***The following cell was generated using openAI***\n",
    "\n",
    "> **Reason**: L. Laroze is a Pok√©mon Master (*someone who really knows how to play*) but C. N√∫√±ez is not. Actually, C. N√∫√±ez does not know anything at all of Pok√©mon. In order to understand the data without learning how to play we ask openAI to give us a complete summary of the different types of classes/columns that is presented. We also looked up the other Series/DataFrames like Disney+ and this kind of approach was similar to asking openAI what is the rating of metascore, imdb_rating, imdb_score, etc. Keeping in mind L. Laroze is more than a veteran in this matter of fact, we can modify the response of openAI if necessary given the talent of the Master.\n",
    "\n",
    "> **Prompt**: Given the following panda Data Frame of python output cell (*I gave the first 5 rows of the cell output and the data.describe() parameters*), tell me the range and description of the columns and rows. What is good and bad? How do I classify the scores? What's the min value and max value of each column?\n",
    "\n",
    "---\n",
    "\n",
    "Perfect ‚Äî now we can rewrite the analysis using the **actual `data.describe()` output** you just provided.\n",
    "\n",
    "---\n",
    "\n",
    "# üìä Pok√©mon DataFrame Summary\n",
    "\n",
    "Your dataset has **801 Pok√©mon entries (rows)** and at least **34 numeric columns** (stats, effectiveness multipliers, height, weight, etc.). Below is a breakdown of ranges, classification, and what ‚Äúgood‚Äù or ‚Äúbad‚Äù means for each type of column.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Type Effectiveness (`against_*` columns)**\n",
    "\n",
    "* **Range:** `0.0 ‚Üí 4.0`\n",
    "* **Meaning:** Multiplier applied to incoming damage of that type.\n",
    "\n",
    "  * `0` ‚Üí **Immune** (best case).\n",
    "  * `0.25‚Äì0.5` ‚Üí **Resistant** (takes reduced damage).\n",
    "  * `1` ‚Üí **Neutral** (normal damage).\n",
    "  * `2‚Äì4` ‚Üí **Weakness** (takes more damage, bad).\n",
    "* **Min/Max (from your data):**\n",
    "\n",
    "  * Min values: 0.0 (e.g. Normal immune to Ghost).\n",
    "  * Max values: 4.0 (quadruple weakness, e.g. Grass/Flying vs Ice).\n",
    "\n",
    "üëâ **Good**: lower values (<1).\n",
    "üëâ **Bad**: higher values (>1).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Base Stats**\n",
    "\n",
    "(Values used in Pok√©mon battles; higher = stronger)\n",
    "\n",
    "* **`hp` (Hit Points)**\n",
    "\n",
    "  * Range: `1 ‚Üí 255`\n",
    "  * Good: ‚â• 100 = bulky, survivable.\n",
    "  * Bad: ‚â§ 50 = frail.\n",
    "\n",
    "> * **`attack` (Attack)**\n",
    ">\n",
    ">  * Range: `5 ‚Üí 185`\n",
    ">  * Good: ‚â• 110 = strong physical attacker.\n",
    ">  * Bad: ‚â§ 50 = weak.\n",
    "\n",
    "> * **`defense` (Defense)**\n",
    ">\n",
    ">  * Range: `5 ‚Üí 230`\n",
    ">  * Good: ‚â• 110 = physical wall.\n",
    ">  * Bad: ‚â§ 50 = weak.\n",
    "\n",
    "* **`sp_attack` (Special Attack)**\n",
    "\n",
    "  * Range: `10 ‚Üí 194`\n",
    "  * Good: ‚â• 110 = strong special attacker.\n",
    "  * Bad: ‚â§ 50 = weak.\n",
    "\n",
    "* **`sp_defense` (Special Defense)**\n",
    "\n",
    "  * Range: `20 ‚Üí 230`\n",
    "  * Good: ‚â• 110 = special wall.\n",
    "  * Bad: ‚â§ 50 = fragile.\n",
    "\n",
    "* **`speed`**\n",
    "\n",
    "  * Range: `5 ‚Üí 180`\n",
    "  * Good: ‚â• 100 = very fast.\n",
    "  * Bad: ‚â§ 50 = very slow.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Physical Traits**\n",
    "\n",
    "* **`height_m`**\n",
    "\n",
    "  * Range: `0.1 ‚Üí 14.5 m`\n",
    "  * Informational only; matters for flavor or some moves (e.g. Low Kick).\n",
    "\n",
    "* **`weight_kg`**\n",
    "\n",
    "  * Range: `0.1 ‚Üí 999.9 kg`\n",
    "  * Heavier Pok√©mon benefit from moves like Heavy Slam but are vulnerable to Low Kick.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Demographics**\n",
    "\n",
    "* **`percentage_male`**\n",
    "\n",
    "  * Range: `0 ‚Üí 100`\n",
    "  * 0 = all female/genderless, 100 = all male.\n",
    "  * Neutral ‚Äî not good/bad, just species trait.\n",
    "\n",
    "* **`generation`**\n",
    "\n",
    "  * Range: `1 ‚Üí 7`\n",
    "  * Indicates introduction era. Not about strength.\n",
    "\n",
    "* **`pokedex_number`**\n",
    "\n",
    "  * Range: `1 ‚Üí 801`\n",
    "  * Pure identifier.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Legendary Status**\n",
    "\n",
    "* **`is_legendary`**\n",
    "\n",
    "  * Values: `0` (non-legendary), `1` (legendary).\n",
    "  * Legendary Pok√©mon generally have higher stats (base stat totals often >600).\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **Summary Table**\n",
    "\n",
    "| Column Group      | Range (min‚Äìmax) | Good / Bad Meaning                      |\n",
    "| ----------------- | --------------- | --------------------------------------- |\n",
    "| `against_*`       | 0.0 ‚Üí 4.0       | <1 = good (resist), >1 = bad (weakness) |\n",
    "| `hp`              | 1 ‚Üí 255         | High = bulky, low = frail               |\n",
    "| `sp_attack`       | 10 ‚Üí 194        | High = strong attacker                  |\n",
    "| `sp_defense`      | 20 ‚Üí 230        | High = durable vs specials              |\n",
    "| `speed`           | 5 ‚Üí 180         | High = moves first                      |\n",
    "| `height_m`        | 0.1 ‚Üí 14.5      | Informational                           |\n",
    "| `weight_kg`       | 0.1 ‚Üí 999.9     | Contextual                              |\n",
    "| `percentage_male` | 0 ‚Üí 100         | Neutral descriptor                      |\n",
    "| `generation`      | 1 ‚Üí 7           | Contextual                              |\n",
    "| `pokedex_number`  | 1 ‚Üí 801         | Identifier only                         |\n",
    "| `is_legendary`    | 0 / 1           | 1 = Legendary Pok√©mon                   |\n",
    "\n",
    "---\n",
    "\n",
    "***This is the end of the cell generated using openAI***\n",
    ">\n",
    "> Some descriptions of the columns where not completely described by openAI, so we added them manually e.g., attack and defense stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b7ec13f3068346ba98dc65f4899dc3f0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> # **Limpieza de datos**\n",
    ">\n",
    "> Identificaci√≥n de valores no definidos o faltantes\n",
    "> - Aquellos valores tipo NaN indican la etiqueta *Not A Number* por lo que es un valor que se desconoce o no est√° definido.\n",
    "> - Por medio de la operaci√≥n **data.info()** es posible obtener informaci√≥n relevante y necesaria para identificar valores tipo NaN:\n",
    "> - Cabe mencionar que no todos los pokemones poseen un tipo 2, por lo que los valores NaN en esta columna no son un problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripci√≥n num√©rica del set de datos\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cfb4f585f6564a1b917a3f0bf8fde9e2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "|index |  parameter       |           number non-null |    type    |\n",
    "|----|------------------|-------------------------------|------------|\n",
    "| 27 |  height_m        |           781 non-null        |    float64 |\n",
    "| 31 |  percentage_male |    703 non-null               |    float64 |\n",
    "| 37 |  type2           |              417 non-null     |    object  |\n",
    "| 38 |  weight_kg       |          781 non-null         |    float64 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['height_m'], bins=50)\n",
    "plt.xlabel('height_m')\n",
    "plt.show()\n",
    "plt.hist(data['weight_kg'], bins=50)\n",
    "plt.xlabel('weight_kg')\n",
    "plt.show()\n",
    "plt.hist(data['percentage_male'], bins=50)\n",
    "plt.xlabel('percentage_male')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0fc515ee47604b75aee1f1e84e947458",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## **Explicaci√≥n de valores NaN:**\n",
    ">\n",
    "> * height_m:\n",
    ">   * 20 valores NaN. \n",
    ">   * Debido a la presencia de outliers, los valores NaN ser√°n reemplazados por la mediana de los datos\n",
    ">\n",
    "> * weight_kg:\n",
    ">   * 20 valores NaN. \n",
    ">   * Seg√∫n la misma l√≥gica, estos valores tambi√©n ser√°n reemplazados por la mediana de los datos\n",
    ">\n",
    "> * percentage_male: \n",
    ">   * 98 valores NaN.\n",
    ">   * No se reemplazaran porque no todos los pok√©mon tienen g√©nero\n",
    ">\n",
    "> * type2:\n",
    ">   * 384 valores NaN. \n",
    ">   * No es necesario reemplazarlos porque no todos los pok√©mon tienen segundo tipo\n",
    ">\n",
    "> En la siguiente l√≠nea reemplazamos los valores NaN mencionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['height_m'].fillna(data['height_m'].median(), inplace=True)\n",
    "data['weight_kg'].fillna(data['weight_kg'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e9956f8427874bf2a3dc8297075eacd6",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## **Filas o columnas duplicadas**\n",
    ">\n",
    "> Afortunadamente observamos que no contamos con filas ni columnas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e714cd0e679f4783b21b9c4a09d71985",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> # **An√°lisis estad√≠stico**\n",
    ">\n",
    "> Para la visualizaci√≥n de datos seg√∫n un An√°lisis Exploratorio de Datos (EDA) se utilizar√° la librer√≠a `seaborn` junto al apoyo de `matplotlib`. Seg√∫n estas librer√≠as contamos con el m√©todo `catplot` que permite visualizar datos categ√≥ricos y `displot` que permite visualizar datos num√©ricos. Adem√°s, se utilizar√° el m√©todo `plot` de `pandas` y el m√©todo `hist` de `matplotlib` para ayudar a comprender la relevancia y diferencia entre sets de datos. \n",
    ">\n",
    "> * Para algunos elementos nos daremos cuenta que es posible analizar y visualizar de manera similar o an√°loga el mismos set de datos empleando histograma u otro medio de visualizaci√≥n gr√°fica.\n",
    "> * El criterio de visulizaci√≥n se basa en la cantidad de datos que se desea visualizar y la forma en que se desea comprender los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "13c7433d4aff45a5b56bbc8c865899a0",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown",
    "execution_context_id": "60beab00-960c-4dff-adde-990e21a2fc8c",
    "execution_millis": 0,
    "execution_start": 1757466491356,
    "source_hash": "ec881b0a"
   },
   "source": [
    "> ## An√°lisis de variables categ√≥ricas\n",
    "> En esta secci√≥n se analizar√°n mediante distintos gr√°ficos las variables categ√≥ricas del set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=data, x='is_legendary', kind='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fc0fb2d983a341c5a1213f539c891ef0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Observamos que la mayor√≠a de los pokemones no son legendarios, por lo que se tiene una distribuci√≥n desbalanceada de datos.\n",
    "> * Es posible representar estos datos como un histograma de dos bins o bien dentro de un gr√°fico de torta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for 1\n",
    "legendary = list(filter(lambda x : x // 1 == 1, list(data['is_legendary'])))\n",
    "print(f'el n√∫mero de pok√©mon legendarios es {len(legendary)}')\n",
    "\n",
    "# Data for 0\n",
    "legendario = data['is_legendary']\n",
    "print(f'el n√∫mero de pok√©mon normales es {len(list(legendario))-len(legendary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: 70 ones, 731 zeros -> two categories\n",
    "Leg = len(legendary)\n",
    "notLeg = len(list(legendario))-len(legendary)\n",
    "x = [notLeg, Leg]\n",
    "labels = [\"Normal\", \"Legendario\"]\n",
    "colors = plt.get_cmap(\"Blues\")(np.linspace(0.4, 0.8, len(x)))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(\n",
    "    x,\n",
    "    labels=labels,\n",
    "    colors=colors,\n",
    "    autopct=\"%1.1f%%\",             # show percentages\n",
    "    startangle=90,                 # start from the top\n",
    "    wedgeprops={\"linewidth\": 1, \"edgecolor\": \"white\"}\n",
    ")\n",
    "\n",
    "ax.set_title(\"Distribuci√≥n de Clases\", fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6cf1cb0b8ce046e691658eca3b973329",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Con esto, podemos ver y discriminar de manera porcentual que la mayor√≠a de los pok√©mon son normales y no legendarios. \n",
    ">\n",
    "> Veamos como se distribuyen estos pok√©mon seg√∫n su tipo principal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3398607027fa42f797750519f9973b4f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> Podemos observar que la mayor√≠a de los Pok√©mon legendarios pertenecen al tipo ‚ÄúPs√≠quico‚Äù. A pesar de esta tendencia en los datos, no es posible generar un modelo de aprendizaje autom√°tico que prediga si un Pok√©mon es legendario o no, debido a que la mayor√≠a de los Pok√©mon no lo son. Sin embargo, un modelo de aprendizaje autom√°tico podr√≠a determinar que un Pok√©mon de tipo Lucha o Veneno no ser√° legendario.\n",
    ">\n",
    "> En el gr√°fico hay otra caracter√≠sitca que se puede observar, y es la poca cantidad de pok√©mon de tipo flying comparado al resto de tipos. Recordemos que los pokemon tienen el \"type1\" pero tambi√©n pueden tener type2, veamos si en este gr√°fico podemos obtener informaci√≥n relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=data, x='type1', kind='count', height=8, aspect=1.5, hue = \"is_legendary\")\n",
    "plt.show()\n",
    "\n",
    "sns.catplot(data=data, x='type2', kind='count', height=8, aspect=1.5, hue = \"is_legendary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "50688e9c679e4a1fadae52444da99ca3",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Con este √∫ltimo plot se encontr√≥ que la mayor√≠a de pok√©mon que presentan el tipo volador lo hacen en su caracter√≠sitca de type2 en vez de type1. \n",
    "> * Esto se puede deber a un simple capricho de los desarrolladores del juego que condiciona a la hora de estudiar los tipos m√°s recurrentes en los pok√©mon. \n",
    "> * Para ver cual es el tipo m√°s usual, debemos hacer una suma directa de ambas caracter√≠sticas referentes a los tipos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipos = []\n",
    "cont = []\n",
    "for a in data[\"type1\"]:\n",
    "    if a not in tipos:\n",
    "        tipos.append(a)\n",
    "        cont.append(1)\n",
    "    else:\n",
    "        cont[tipos.index(a)] += 1\n",
    "tip2 = data[\"type2\"]\n",
    "for b in tip2:\n",
    "    if type(b) == str:\n",
    "        if b not in tipos:\n",
    "            tipos.append(b)\n",
    "            cont.append(1)\n",
    "        else:\n",
    "            cont[tipos.index(b)] += 1\n",
    "print(tipos)\n",
    "print(cont)\n",
    "plt.figure(figsize = (14,8))\n",
    "plt.bar(tipos, cont, color='blue')\n",
    "plt.xlabel('Tipos')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.title('Cantidad de Pok√©mon por Tipo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b7797ca1aa294b2cbb905b65d19e9403",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Con esto finalmente podemos ver que tipo de pokemon es m√°s com√∫n, lo que puede ser relevante para un futuro modelo de machine learning utilizando varibles categ√≥ricas.\n",
    "\n",
    "> Otra pregunta que puede surgir es: ¬øHay pok√©mon que sean siempre machos o siempre hembra?\n",
    "> Para responder esto, analizaremos la variable percentage_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poke_sex(sex):\n",
    "    if sex < 50:\n",
    "        return 'primarly female'\n",
    "    elif sex > 50:\n",
    "        return 'primarly male'\n",
    "    else:\n",
    "        return 'no predilection'\n",
    "\n",
    "data['percentage_male'] = data.apply(lambda x: poke_sex(x['percentage_male']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=data, x='percentage_male', kind='count', height=8, aspect=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "71f064150c244a6eb869e1e2ed4d7641",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> Aqu√≠ podemos observar como la mayor√≠a de los pok√©mon no tienen predilecci√≥n de g√©nero, sin embargo, hay algunos pok√©mon que son mayormente machos o hembras. Creemos que esto debe ser por algo de car√°cter estad√≠stico y no por azar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cc37525d59d34fdeb0a57a2b78b68ebe",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## An√°lisis de variables no categ√≥ricas\n",
    "> En esta secci√≥n se analizar√°n mediante histogramas y otras visualizaciones las variables no categ√≥ricas del set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hp'].plot(title='Vida base', kind='hist', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(data['hp'].mean(), 2) # calculo de la media de la vida base de los pok√©mon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6e3534006bdb41f0b3328404e7e156a1",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * En algunos casos es posible identificar outliers en los datos, por lo que se debe tener cuidado al momento de realizar an√°lisis estad√≠sticos.\n",
    "> * En este caso, se observa que la vida base de los pok√©mon se distribuye de manera similar a una distribuci√≥n normal, con una media de 69.3 puntos de vida base.\n",
    "> * Es m√°s f√°cil identificar outliers dentro de un digrama de caja y bigote pues se puede identificar el rango intercuart√≠lico y los valores at√≠picos.\n",
    "> * Vamos a realizar diagramas de caja y bigote para las estad√≠sticas base de los pok√©mon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['hp'].mean().round(2), data['hp'].median())\n",
    "data['hp'].plot(title='Vida Inicial', kind='box')\n",
    "plt.hlines(y=data['hp'].mean(), xmin=0, xmax=2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['hp'].plot(title='Vida Inicial', kind='box')\n",
    "data['hp'].plot(title='Vida Inicial', kind='box', ylim=(5,130))\n",
    "plt.hlines(y=data['hp'].mean(), xmin=0, xmax=2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['attack'].plot(title='Ataque base', kind='box')\n",
    "plt.hlines(y=data['attack'].mean(), xmin=0, xmax=2, color='red')\n",
    "#data['hp'].plot(title='Vida Inicial', kind='box', ylim=(5,130))\n",
    "plt.show()\n",
    "print(\"el promedio de ataque es\", round(data['attack'].mean(), 2), \"y la mediana es\", data['attack'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['defense'].plot(title='Defensa base', kind='box')\n",
    "plt.hlines(y=data['defense'].mean(), xmin=0, xmax=2, color='red')\n",
    "#data['hp'].plot(title='Defensa Base', kind='box', ylim=(5,130))\n",
    "plt.show()\n",
    "print(\"el promedio de defensa es\", data['defense'].mean().round(2), \"y la mediana es\", data['defense'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sp_attack'].plot(title='Ataque especial base', kind='box')\n",
    "plt.hlines(y=data['sp_attack'].mean(), xmin=0, xmax=2, color='red')\n",
    "#data['hp'].plot(title='Vida Inicial', kind='box', ylim=(5,130))\n",
    "plt.show()\n",
    "print(\"el promedio de ataque especial es\", data['sp_attack'].mean().round(2), \"y la mediana es\", data['sp_attack'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sp_defense'].plot(title='Defensa especial base', kind='box')\n",
    "plt.hlines(y=data['sp_defense'].mean(), xmin=0, xmax=2, color='red')\n",
    "#data['hp'].plot(title='Vida Inicial', kind='box', ylim=(5,130))\n",
    "plt.show()\n",
    "print(\"el promedio de defensa especial es\", data['sp_defense'].mean().round(2), \"y la mediana es\", data['sp_defense'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speed'].plot(title='Velocidad base', kind='box')\n",
    "plt.hlines(y=data['speed'].mean(), xmin=0, xmax=2, color='red')\n",
    "#data['hp'].plot(title='Vida Inicial', kind='box', ylim=(5,130))\n",
    "plt.show()\n",
    "print(\"el promedio de velocidades es\", data['speed'].mean().round(2), \"y la mediana es\", data['speed'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "025907071b8c45c2aa710c6b67b70c09",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * En los diagramas anteriores podemos notar como todas las estad√≠sitcas tienen outliers presentes.\n",
    "> * Estos outliers generalmente se encuentran en las estad√≠sticas m√°s altas, lo que se ve reflejado en la diferencia entre la mediana y el promedio de los datos, donde la mediana siempre es menor que el promedio.\n",
    "> * Este comportamiento implica que si hubiesemos ten√≠amos datos faltantes los deber√≠amos reemplazar por la mediana de los datos y no por la media.\n",
    "> * Por √∫ltimo, podemos notar que el comportamiento estat√≠stico de las diferentes estad√≠sticas es similar, y para observar esto mejor, hacemos un gr√°fico de densidad de probabilidad de estas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['attack'].plot(kind='density', label='Ataque')\n",
    "data['defense'].plot(kind='density', label='Defensa')\n",
    "data['sp_attack'].plot(kind='density', label='Ataque especial')\n",
    "data['sp_defense'].plot(kind='density', label='Defensa especial')\n",
    "data['speed'].plot(kind='density', label='Velocidad')\n",
    "plt.legend(title='Estad√≠sticas')\n",
    "plt.xlim(0,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_mean = data['attack'].mean()\n",
    "attack_std = data['attack'].std()\n",
    "data['attack'].plot(kind='density')\n",
    "plt.axvspan(0, attack_mean-attack_std, alpha=0.3, color='red', label='Bad')\n",
    "plt.axvspan(attack_mean-attack_std, attack_mean+attack_std, alpha=0.3, color='orange', label='Regular')\n",
    "plt.axvspan(attack_mean+attack_std, 200, alpha=0.3, color='green', label='Good')\n",
    "plt.xlim(0,200)\n",
    "plt.legend(title='Performace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defense_mean = data['defense'].mean()\n",
    "defense_std = data['defense'].std()\n",
    "data['defense'].plot(kind='density')\n",
    "plt.axvspan(0, defense_mean-defense_std, alpha=0.3, color='red', label='Bad')\n",
    "plt.axvspan(defense_mean-defense_std, defense_mean+defense_std, alpha=0.3, color='orange', label='Regular')\n",
    "plt.axvspan(defense_mean+defense_std, 230, alpha=0.3, color='green', label='Good')\n",
    "plt.xlim(0,230)\n",
    "plt.legend(title='Performace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_attack_mean = data['sp_attack'].mean()\n",
    "sp_attack_std = data['sp_attack'].std()\n",
    "data['sp_attack'].plot(kind='density')\n",
    "plt.axvspan(0, sp_attack_mean-sp_attack_std, alpha=0.3, color='red', label='Bad')\n",
    "plt.axvspan(sp_attack_mean-sp_attack_std, sp_attack_mean+sp_attack_std, alpha=0.3, color='orange', label='Regular')\n",
    "plt.axvspan(sp_attack_mean+sp_attack_std, 210, alpha=0.3, color='green', label='Good')\n",
    "plt.xlim(0,210)\n",
    "plt.legend(title='Performace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_defense_mean = data['sp_defense'].mean()\n",
    "sp_defense_std = data['sp_defense'].std()\n",
    "data['sp_defense'].plot(kind='density')\n",
    "plt.axvspan(0, sp_defense_mean-sp_defense_std, alpha=0.3, color='red', label='Bad')\n",
    "plt.axvspan(sp_defense_mean-sp_defense_std, sp_defense_mean+sp_defense_std, alpha=0.3, color='orange', label='Regular')\n",
    "plt.axvspan(sp_defense_mean+sp_defense_std, 230, alpha=0.3, color='green', label='Good')\n",
    "plt.xlim(0,230)\n",
    "plt.legend(title='Performace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_mean = data['speed'].mean()\n",
    "speed_std = data['speed'].std()\n",
    "data['speed'].plot(kind='density')\n",
    "plt.axvspan(0, speed_mean-speed_std, alpha=0.3, color='red', label='Bad')\n",
    "plt.axvspan(speed_mean-speed_std, speed_mean+speed_std, alpha=0.3, color='orange', label='Regular')\n",
    "plt.axvspan(speed_mean+speed_std, 200, alpha=0.3, color='green', label='Good')\n",
    "plt.xlim(0,200)\n",
    "plt.legend(title='Performace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1b83ae5e3a834cfba85d1af16f76e084",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Es importante mencionar que no se realiz√≥ un ajuste de curva exhaustivo para determinar la distribuci√≥n de los datos, por lo que se debe tener cuidado al interpretar los resultados.\n",
    "> * Para mejorar el rango estad√≠stico el cual se considera \"bueno\", \"malo\" o \"regular\" ser√≠a factible poder realizar un ajuste de curva Voight, Lorenztiano o bien Gausseano para determinar una desviaci√≥n est√°ndar real de los datos.\n",
    "> * El criterio empleado en este gr√°fico fue el uso de la desviaci√≥n est√°ndar de los datos, donde se considera que los datos que se encuentran a una desviaci√≥n est√°ndar de la media son \"regulares\", los que se encuentran a dos desviaciones est√°ndar de la media son \"buenos\" y los que se encuentran a m√°s de dos desviaciones est√°ndar de la media son \"malos\".\n",
    ">\n",
    "> ## **Ajuste de curva Gausseano**\n",
    ">\n",
    "> *This code was generated using openAI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "\n",
    "s = data['speed'].dropna().to_numpy()\n",
    "\n",
    "# Fit Gaussian (mean, std)\n",
    "mu, sigma = norm.fit(s)\n",
    "\n",
    "# Build density curves\n",
    "kde = gaussian_kde(s)\n",
    "x = np.linspace(0, 200, 1000)\n",
    "\n",
    "# --- Plot ---\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# KDE (empirical density)\n",
    "ax.plot(x, kde(x), lw=2, label='KDE Density')\n",
    "\n",
    "# Gaussian PDF (fit)\n",
    "ax.plot(x, norm.pdf(x, mu, sigma), 'r--', lw=2, label=f'Gaussian Fit (Œº={mu:.1f}, œÉ={sigma:.1f})')\n",
    "\n",
    "# Shaded regions (same as before)\n",
    "ax.axvspan(0, mu - sigma, alpha=0.3, color='red', label='Bad')\n",
    "ax.axvspan(mu - sigma, mu + sigma, alpha=0.3, color='orange', label='Regular')\n",
    "ax.axvspan(mu + sigma, 200, alpha=0.3, color='green', label='Good')\n",
    "\n",
    "# Vertical lines for ¬±1œÉ and ¬±2œÉ\n",
    "for k, style in zip([1, 2], ['--', ':']):\n",
    "    ax.axvline(mu - k*sigma, color='black', linestyle=style, alpha=0.7)\n",
    "    ax.axvline(mu + k*sigma, color='black', linestyle=style, alpha=0.7)\n",
    "\n",
    "ax.set_xlim(0, 200)\n",
    "ax.set_xlabel('Speed')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Speed Distribution with Gaussian Fit')\n",
    "ax.legend(title='Performance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e906c4968f7b4a5d83674e4eb7fa970b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This following cell was generated using openAI*\n",
    "\n",
    "**Reason**: We had a problem understanding why the std of the fit was the same as the std of the data. We asked openAI to explain why this was happening.\n",
    "\n",
    "Good catch üëå ‚Äî this happens because of **what you mean by ‚Äúfit.‚Äù**\n",
    "\n",
    "---\n",
    "\n",
    "## üîé Why they match\n",
    "\n",
    "When you do this:\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "mu, sigma = norm.fit(data)\n",
    "```\n",
    "\n",
    "* `norm.fit` uses **Maximum Likelihood Estimation (MLE)** to estimate the Gaussian parameters.\n",
    "* For a *true normal distribution*, the MLE for the mean ($\\mu$) is the **sample mean**, and the MLE for the standard deviation ($\\sigma$) is **almost** the sample standard deviation.\n",
    "\n",
    "So:\n",
    "\n",
    "* **Mean**: exactly equal to the sample mean.\n",
    "* **Standard deviation**:\n",
    "\n",
    "  * The usual *pandas* `data.std()` uses the **unbiased estimator** (divides by $n-1$).\n",
    "  * `norm.fit` (MLE) uses the **biased estimator** (divides by $n$).\n",
    "\n",
    "For large samples, the difference is tiny, so they look ‚Äúthe same.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è The math\n",
    "\n",
    "* Sample mean:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n x_i\n",
    "  $$\n",
    "\n",
    "* Sample variance (unbiased, what `pandas.std()` does):\n",
    "\n",
    "  $$\n",
    "  s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\hat{\\mu})^2\n",
    "  $$\n",
    "\n",
    "* MLE variance (used in `norm.fit`):\n",
    "\n",
    "  $$\n",
    "  \\sigma^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\hat{\\mu})^2\n",
    "  $$\n",
    "\n",
    "The difference is only the denominator ($n$ vs $n-1$).\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è How to check in Python\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "s = data['speed'].dropna().to_numpy()\n",
    "\n",
    "# Pandas/NumPy std (unbiased, denominator = n-1)\n",
    "print(\"Sample std (unbiased):\", np.std(s, ddof=1))\n",
    "\n",
    "# Population std (biased, denominator = n)\n",
    "print(\"Sample std (MLE form):\", np.std(s, ddof=0))\n",
    "\n",
    "# Fit with scipy\n",
    "mu_fit, sigma_fit = norm.fit(s)\n",
    "print(\"Sigma from norm.fit:\", sigma_fit)\n",
    "```\n",
    "\n",
    "You‚Äôll see:\n",
    "\n",
    "* `np.std(s, ddof=0)` ‚âà `sigma_fit`\n",
    "* `np.std(s, ddof=1)` is slightly larger\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **So in short:**\n",
    "The Gaussian ‚Äúfit‚Äù and the raw data std match because the MLE estimate of œÉ is basically the same as the dataset‚Äôs standard deviation (with denominator $n$). The fit isn‚Äôt doing anything fancier ‚Äî it just reproduces the sample statistics.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Do you want me to also explain what happens if your data **isn‚Äôt Gaussian** (skewed or heavy-tailed) ‚Äî i.e. how the ‚Äúfit œÉ‚Äù can then differ significantly from the raw sample std?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "s = data['speed'].dropna().to_numpy()\n",
    "\n",
    "# Pandas/NumPy std (unbiased, denominator = n-1)\n",
    "print(\"Sample std (unbiased):\", np.std(s, ddof=1))\n",
    "\n",
    "# Population std (biased, denominator = n)\n",
    "print(\"Sample std (MLE form):\", np.std(s, ddof=0))\n",
    "\n",
    "# Fit with scipy\n",
    "mu_fit, sigma_fit = norm.fit(s)\n",
    "print(\"Sigma from norm.fit:\", sigma_fit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9424abdd635242939e661e0e4244018c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## **Comentarios acerca del ajuste Gaussiano**\n",
    ">\n",
    "> * El ajuste Gaussiano es una herramienta √∫til para determinar la distribuci√≥n de los datos y poder comparar la(s) diferencias que existe con la estimaci√≥n inicial propuesta: utilizar la desviaci√≥n est√°ndar de la muestra para determinar qu√© regiones de la poblaci√≥n pueden ser consideradas como \"buenas\", \"malas\" o \"regulares\".\n",
    "> * En este caso, el ajuste Gaussiano no presenta diferencias significativas con la estimaci√≥n inicial propuesta, por lo que se puede concluir que la distribuci√≥n de los datos es aproximadamente normal y que la discriminaci√≥n tabulada es v√°lida.\n",
    "> * Este modelo ayuda a determinar la desviaci√≥n est√°ndar de la poblaci√≥n, lo que es √∫til para determinar la regi√≥n de \"buenos\" pok√©mon, as√≠ como futuros modelos de machine learning para las pr√≥ximas generaciones de pok√©mon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "43eae1963d8f4731a2c9b1dfedb7beee",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## **An√°lisis de la suma total de atributos**\n",
    ">\n",
    "> * Realizaremos un an√°lisis de la suma total de todos los atributos de los pok√©mon para ver si es posible identificar alg√∫n patr√≥n.\n",
    "> * Para esto, se sumar√°n los atributos de cada pok√©mon y se graficar√° la suma total de los atributos vs la vida base de los pok√©mon.\n",
    "> * Se espera que los pok√©mon legendarios tengan una suma total de atributos mayor a los pok√©mon no legendarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df['stats'] = df['hp'] + df['attack'] + df['defense'] + df['sp_attack'] + df['sp_defense'] + df['speed']\n",
    "\n",
    "plt.xlabel('Estad√≠sticas')\n",
    "plt.ylabel('Vida')\n",
    "plt.title('Vida vs Estad√≠sticas')\n",
    "\n",
    "c1 = True\n",
    "c2 = True\n",
    "for i in range(len(df)):\n",
    "    if df['is_legendary'][i] == 1:\n",
    "        if c1 == True:\n",
    "            plt.scatter(df['stats'][i], df['hp'][i], color='red', label = \"Legendario\")\n",
    "            c1 = False\n",
    "        plt.scatter(df['stats'][i], df['hp'][i], color='red')\n",
    "    else:\n",
    "        if c2 == True:\n",
    "            plt.scatter(df['stats'][i], df['hp'][i], color='blue', label='No legendario')\n",
    "            c2 = False\n",
    "        plt.scatter(df['stats'][i], df['hp'][i], color='blue')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ebb35be073274dbfa5b8e222c71d0f04",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Notamos claramente que hay  una tendencia de que los pok√©mon legendarios tengan una suma total de atributos mayor a los pok√©mon no legendarios, con unos pocos outliers legendarios. Para una visualizaci√≥n m√°s atractiva, hacemos un plot de densidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "16b29f8a13d54e9cbb9e069102ff52ee",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ### Visualizaci√≥n de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "48dbb404a6cf490b94efddb94b3a91ae",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This code was generated using openAI*\n",
    "\n",
    "> **Reason**: The code was generated using openAI because it was not possible to generate the code without the use of openAI. The code was generated using the following prompt: \"Generate a code that plots a kdeplot of the stats column of the df dataframe, using the is_legendary column as the hue. The x axis should be limited to the min and max values of the stats column. The plot should be a FacetGrid with an aspect of 5. The plot should be shown using plt.show()\"\n",
    "\n",
    "> * Es crucial considerar que un gr√°fico de densidad es una herramienta efectiva para visualizar la distribuci√≥n de los datos, ya que muestra la funci√≥n de densidad de probabilidad normalizada i.e., la integral de dicha funci√≥n equivalente a la unidad. A diferencia de un gr√°fico de dispersi√≥n no se muestran los puntos en particular, sino una tendencia general de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Correcting the error by using the correct column name 'stats' in the FacetGrid map function\n",
    "as_fig = sns.FacetGrid(df, hue='is_legendary', aspect=5)\n",
    "as_fig.map(sns.kdeplot, 'stats', fill=True)\n",
    "\n",
    "max_value = df['stats'].max()\n",
    "min_value = df['stats'].min()\n",
    "as_fig.set(xlim=(min_value, max_value))\n",
    "as_fig.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "76efa2c54a044465a492ec1bba802d31",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Con las funciones de densidad de probabilidad se puede ver de una mejor manera la tendencia de los legendarios a tener estad√≠sticas altas, mientras que los no legendarios presentan una distribuci√≥n m√°s uniforme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b66961d321e04af88e468534bd6ffd65",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ### An√°lisis de correlaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fix the error, we need to provide another Series to calculate the correlation with.\n",
    "# For example, if we want to calculate the correlation between 'stats' and 'hp':\n",
    "\n",
    "correlation = df['stats'].corr(df['hp'])\n",
    "print(\"Correlation between stats and hp:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.copy()[['attack', 'base_egg_steps', 'defense', 'experience_growth', 'hp', 'percentage_male', 'pokedex_number', 'sp_attack', 'sp_defense', 'speed', 'generation', 'is_legendary', 'stats']]\n",
    "print(corr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0cd829b0f36a416a9379ebcf76a59fab",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This code was generated using openAI*\n",
    "\n",
    "> **Reason**: The code was generated using openAI because it was not possible to generate the code without the use of openAI. The code was generated using the following prompt: \"Generate a code that plots a heatmap of the correlation matrix of the corr_df dataframe. The heatmap should have a size of 15x12, the annotations should be formatted to 2 decimal places, the colormap should be 'coolwarm' and the center of the colormap should be 0. The title of the heatmap should be 'Correlation Matrix' with a font size of 14. The plot should be shown using plt.show()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fix the error, we need to exclude non-numeric columns from the DataFrame before calculating the correlation matrix.\n",
    "\n",
    "# Select only numeric columns from the DataFrame\n",
    "numeric_df = corr_df.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate the correlation matrix for the numeric columns\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Matrix\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4507dc96357340c79d4caa2b75c9faa9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This following prompt was generated using openAI*\n",
    "\n",
    "Yes ‚Äî there are several good options depending on how you want to *calculate* and how you want to *visualize* the correlation matrix. Let‚Äôs go step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Calculate correlation matrix**\n",
    "\n",
    "The standard way is with **pandas**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "corr = df.corr()   # by default: Pearson correlation\n",
    "```\n",
    "\n",
    "You now have a DataFrame with the pairwise correlations.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Visualize with Seaborn**\n",
    "\n",
    "Seaborn doesn‚Äôt calculate correlations itself, but it‚Äôs *great for plotting* them.\n",
    "Most common tool: `sns.heatmap`\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Matrix\", fontsize=14)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "* `annot=True` ‚Üí show values inside the cells.\n",
    "* `center=0` ‚Üí colors diverge around zero.\n",
    "* `cmap=\"coolwarm\"` ‚Üí blue = negative, red = positive.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Alternative visualization libraries**\n",
    "\n",
    "* **`seaborn.clustermap`**\n",
    "  Automatically clusters correlated variables for structure discovery:\n",
    "\n",
    "  ```python\n",
    "  sns.clustermap(corr, annot=True, cmap=\"coolwarm\", center=0)\n",
    "  ```\n",
    "* **`matplotlib` (manual)** ‚Üí use `plt.matshow` or `ax.imshow`.\n",
    "* **`pingouin`** (a stats library) ‚Üí has `pingouin.pairwise_corr` for more advanced correlations.\n",
    "* **`plotly`** ‚Üí interactive heatmaps if you want hover info:\n",
    "\n",
    "  ```python\n",
    "  import plotly.express as px\n",
    "  fig = px.imshow(corr, text_auto=True, color_continuous_scale=\"RdBu_r\", zmin=-1, zmax=1)\n",
    "  fig.show()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Best practice\n",
    "\n",
    "* **pandas** for computing (`.corr()`).\n",
    "* **seaborn.heatmap** for a clean, static plot.\n",
    "* **plotly** if you need interactive exploration.\n",
    "\n",
    "---\n",
    "\n",
    "Good ‚Äî let‚Äôs interpret that number carefully.\n",
    "\n",
    "A correlation coefficient (usually Pearson‚Äôs *r*) ranges from **‚Äì1 to +1**:\n",
    "\n",
    "* **+1** ‚Üí perfect positive linear relationship (as X goes up, Y goes up in lockstep).\n",
    "* **0** ‚Üí no linear relationship.\n",
    "* **‚Äì1** ‚Üí perfect negative linear relationship (as X goes up, Y goes down in lockstep).\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Your case: **r = 0.6206**\n",
    "\n",
    "* **Direction**: Positive ‚Üí as one variable increases, the other tends to increase.\n",
    "* **Strength**: 0.62 is generally interpreted as a **moderately strong** correlation.\n",
    "\n",
    "  * Rough scale (rule of thumb, varies by field):\n",
    "\n",
    "    * 0.0‚Äì0.2 ‚Üí negligible\n",
    "    * 0.2‚Äì0.4 ‚Üí weak\n",
    "    * 0.4‚Äì0.7 ‚Üí moderate\n",
    "    * 0.7‚Äì0.9 ‚Üí strong\n",
    "    * 0.9‚Äì1.0 ‚Üí very strong / almost deterministic\n",
    "\n",
    "So: **a moderately strong positive linear relationship**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä What it implies\n",
    "\n",
    "* The two variables share a noticeable trend, but it‚Äôs **not perfect**.\n",
    "* You could use one to *partially* predict the other, but there‚Äôs still a lot of variance unexplained.\n",
    "\n",
    "Formally, the **coefficient of determination** (*r¬≤*) = (0.6206)¬≤ ‚âà **0.385**.\n",
    "That means about **38.5% of the variance** in one variable can be explained by a linear relationship with the other. The remaining \\~61.5% is noise or other factors.\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Caution**:\n",
    "\n",
    "* Correlation ‚â† causation.\n",
    "* Nonlinear relationships might not show up well in Pearson‚Äôs *r*.\n",
    "* Outliers can inflate or deflate the value.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "55325f0d2f704e078758f395ed73b9a2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## **Comentarios sobre la matriz de correlaci√≥n obtenida**\n",
    ">\n",
    "> **Correlaci√≥n entre vida base y suma total de atributos**\n",
    ">\n",
    "> * Es posible observar que la correlaci√≥n entre la vida base y la suma total de atributos es de 0.62, lo que indica que existe una relaci√≥n lineal moderada entre estas dos variables.\n",
    "> * Esto se debe a que los pok√©mon legendarios tienen una suma total de atributos mayor a los pok√©mon no legendarios.\n",
    "> * Por lo tanto, es posible que los pok√©mon legendarios tengan una vida base mayor a los no legendarios.\n",
    ">\n",
    "> **Correlaci√≥n entre pokedex_number y generaci√≥n**\n",
    ">\n",
    "> * Es posible observar que la correlaci√≥n entre el pokedex_number y la generaci√≥n es de 0.99, lo que indica que existe una relaci√≥n lineal cuasi-perfecta entre estas dos variables.\n",
    "> * Esto se debe a que los pok√©mon se ordenan por generaci√≥n y por n√∫mero de pokedex, donde los pok√©mon de la generaci√≥n 1 corresponden a n√∫meros de 1 a 151, los de generaci√≥n 2 de 152 a 251 y as√≠ aumentando, por lo que es l√≥gico que exista una relaci√≥n lineal entre estas dos variables. \n",
    "> * No es de exactamente 1.00 porque el n√∫mero de pok√©mon es diferente en cada generaci√≥n.\n",
    ">\n",
    "> **Correlaci√≥n entre is_legendary y stats**\n",
    ">\n",
    "> * Se observa que la correlaci√≥n entre estas variables es de 0.49, lo que indica que existe una relaci√≥n lineal de 0.25 i.e., 25% de tendencia lineal entre estas dos variables.\n",
    "> * Este bajo porcentaje no proporciona informaci√≥n de la tendencia de encontrar pok√©mon legendarios con altas estad√≠sticas (como si lo hace el plot de densidad), sino que la correlaci√≥n entre las variables no es estrictamente lineal.\n",
    "> * Para realizar un estudio espec√≠fico entre la tendencia de encontrar pok√©mon legendarios con altas estad√≠sticas se debe realizar un an√°lisis de regresi√≥n lineal o bien un an√°lisis de regresi√≥n log√≠stica.\n",
    ">\n",
    "> **Correlaci√≥n entre is_legendary y base_egg_steps**\n",
    ">\n",
    "> * Se observa una correlaci√≥n sorprendentemente alta entre las variables base_egg_steps e is_legendary. \n",
    "> * Para entenderla, debemos saber que base_egg_steps esta relacionado con la crianza que el jugador puede hacer en los juegos de pok√©mon, donde recibes huevos de los cuales nacer√°n estos. Entonces la correlaci√≥n alta implica que se requiere de una mayor cantidad de pasos para que eclosionen los huevos lo que mantiene el status de pok√©mon legendarios.\n",
    ">\n",
    "> **Correlaci√≥n de percentage_male**\n",
    ">\n",
    "> * Gracias a la matriz de correlaci√≥n podemos fijarnos en que valores pueden afectar a la probabilidad de que un pok√©mon sea macho o hembra. Notemos que en realidad, no hay ning√∫n dato que se correlacione directamente con esta probabilidad. La √∫nica que sobresale un poco es la relaci√≥n entre attack y percentage_male, lo que podr√≠a llegar a decir que los pok√©mon m√°s ofensivos suelen ser machos, pero no la correlaci√≥n no es lo suficientemente fuerte como para concluir algo relevante.\n",
    ">\n",
    "> **Correlaci√≥n de defense y sp_defense**\n",
    ">\n",
    "> * En general podemos atribuir una relaci√≥n entre estas dos variables, lo que podr√≠a llegar a decir que los pok√©mon que tienen una defensa f√≠sica alta suelen tener una defensa especial alta i.e., son buenos defensores en general.\n",
    "> * Esto podr√≠a llegar a ser relevante para un futuro modelo de machine learning.\n",
    ">\n",
    "> **Correlaci√≥n entre attack y sp_attack**\n",
    ">\n",
    "> * A diferencia de la defensa, no se puede concluir o relacionar linealmente el ataque f√≠sico con el ataque especial. No es esperable que si un pok√©mon tenga un ataque f√≠sico alto, tenga a su vez un ataque especial alto.\n",
    "> * Esto podr√≠a llegar a ser relevante para un futuro modelo de machine learning.\n",
    ">\n",
    "> **Correlaci√≥n entre caracter√≠sticas de ataque y defensa en relaci√≥n a los stats totales**\n",
    ">\n",
    "> * En general observamos una tendencia trivial respecto a la linealidad entre los par√°metros que componen los stats y los stats totales. Esto es esperable, ya que los stats totales son la suma de los par√°metros que los componen.\n",
    "\n",
    "*The following english cells were generated using openAI*\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 1. Linear Regression\n",
    "\n",
    "* **Goal**: predict a **continuous value**.\n",
    "  Example: predicting **height** in cm from age, or the **price of a house** from its square meters.\n",
    "* **Equation**:\n",
    "\n",
    "  $$\n",
    "  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\varepsilon\n",
    "  $$\n",
    "\n",
    "  where $y$ is a continuous variable (real numbers).\n",
    "* **Assumptions**:\n",
    "\n",
    "  * Linear relationship between predictors and the dependent variable.\n",
    "  * Residuals are normally distributed with constant variance (homoscedasticity).\n",
    "* **Output**: a real number (can be negative, fractional, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ 2. Logistic Regression\n",
    "\n",
    "* **Goal**: predict a **probability** of belonging to a class, usually binary (0 or 1).\n",
    "  Example: Will a student pass (yes/no)? Is an email spam (yes/no)?\n",
    "* **Equation** (uses the sigmoid function to constrain values between 0‚Äì1):\n",
    "\n",
    "  $$\n",
    "  P(y=1 \\mid x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots)}}\n",
    "  $$\n",
    "* **Assumptions**:\n",
    "\n",
    "  * The dependent variable is categorical (often binary).\n",
    "  * Predictors have a linear relationship with the logit (log-odds).\n",
    "* **Output**:\n",
    "\n",
    "  * A **probability between 0 and 1**.\n",
    "  * If you apply a threshold (e.g., 0.5), it becomes a classification (0 or 1).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Key Differences\n",
    "\n",
    "| Aspect             | Linear Regression                        | Logistic Regression               |\n",
    "| ------------------ | ---------------------------------------- | --------------------------------- |\n",
    "| Dependent variable | Continuous                               | Categorical (binary, multinomial) |\n",
    "| Output             | Real number                              | Probability (0‚Äì1)                 |\n",
    "| Model function     | Linear (straight line)                   | Sigmoid / logit                   |\n",
    "| Estimation method  | Least squares (MSE)                      | Maximum likelihood                |\n",
    "| Typical use        | Predicting values (prices, measurements) | Classification (yes/no, 0/1)      |\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Short version:\n",
    "\n",
    "* If your **Y is continuous numeric** ‚Üí use **linear regression**.\n",
    "* If your **Y is categorical (0/1)** ‚Üí use **logistic regression**.\n",
    "\n",
    "---\n",
    "\n",
    "> * Por otro lado, es suficiente realizar un gr√°fico de densidad de probabilidad para visualizar la distribuci√≥n de los datos y la relaci√≥n entre las variables tal y como se mostr√≥ anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ab04b8d878c14ba98715e5f083d01cda",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7a19a6d1f2e34be686149ddc27e0a3e4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This code was generated using openAI*\n",
    "\n",
    ">**Reason**: Esta parte del c√≥digo fue mejorada utilizando ChatGPT-5 debido a la mejora de visualizaci√≥n de los par√°metros importantes y relevantes escritos de manera ordenada y clara.\n",
    "\n",
    "---\n",
    "\n",
    "Durante este an√°lisis exploratorio de datos (EDA) del set de datos de Pok√©mon, se realizaron diversas tareas para comprender mejor las caracter√≠sticas y patrones presentes en los datos. A continuaci√≥n, se resumen los hallazgos m√°s relevantes:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Descripci√≥n y limpieza de datos**\n",
    "\n",
    "* El dataset contiene **801 entradas y 41 columnas**, incluyendo estad√≠sticas base, tipos de Pok√©mon y caracter√≠sticas demogr√°ficas.\n",
    "* Se identificaron valores faltantes en las columnas `height_m`, `weight_kg`, `percentage_male` y `type2`.\n",
    "\n",
    "  * Los valores faltantes en `height_m` y `weight_kg` fueron reemplazados por la **mediana**.\n",
    "  * Los de `percentage_male` y `type2` no fueron modificados debido a su naturaleza inherente.\n",
    "* No se encontraron filas duplicadas en el dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **An√°lisis de variables categ√≥ricas**\n",
    "\n",
    "* La mayor√≠a de los Pok√©mon **no son legendarios** (731 normales vs. 70 legendarios).\n",
    "* Los tipos m√°s comunes son `water`, `normal` y `grass`, mientras que el tipo `flying` es m√°s frecuente como tipo secundario (`type2`).\n",
    "* La distribuci√≥n de g√©nero mostr√≥ que la mayor√≠a de los Pok√©mon no tienen predilecci√≥n de g√©nero, aunque algunos son predominantemente machos o hembras.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **An√°lisis de variables num√©ricas**\n",
    "\n",
    "* Las estad√≠sticas base (`hp`, `attack`, `defense`, `sp_attack`, `sp_defense`, `speed`) presentan **distribuciones similares**, con *outliers* en los valores altos.\n",
    "* Se utiliz√≥ la **desviaci√≥n est√°ndar** para clasificar las estad√≠sticas como \"buenas\", \"regulares\" o \"malas\".\n",
    "* Un **ajuste Gaussiano** confirm√≥ que las distribuciones de las estad√≠sticas son aproximadamente normales.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **An√°lisis de correlaci√≥n**\n",
    "\n",
    "* Se encontr√≥ una correlaci√≥n **moderada (0.62)** entre la vida base (`hp`) y la suma total de estad√≠sticas (`stats`).\n",
    "* La correlaci√≥n entre el n√∫mero de Pok√©dex y la generaci√≥n es **casi perfecta (0.99)**, lo cual es esperable debido al orden en que se introducen los Pok√©mon.\n",
    "* Los Pok√©mon legendarios tienden a tener **estad√≠sticas totales m√°s altas**, pero la correlaci√≥n entre `is_legendary` y `stats` es **moderada (0.49)**, indicando que no todos los Pok√©mon con estad√≠sticas altas son legendarios.\n",
    "* Se observ√≥ una **correlaci√≥n alta** entre `base_egg_steps` e `is_legendary`, lo que sugiere que los Pok√©mon legendarios requieren m√°s pasos para eclosionar.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Visualizaci√≥n de datos**\n",
    "\n",
    "* Se utilizaron gr√°ficos de densidad, diagramas de caja y bigote, gr√°ficos de barras y mapas de calor para visualizar las distribuciones y relaciones entre las variables.\n",
    "* Los gr√°ficos de densidad mostraron claramente la tendencia de los Pok√©mon legendarios a tener estad√≠sticas m√°s altas.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Conclusiones generales**\n",
    "\n",
    "* El dataset est√° bien estructurado y permite identificar patrones claros, como la relaci√≥n entre las estad√≠sticas y el estatus legendario.\n",
    "* Las **estad√≠sticas base y los tipos de Pok√©mon** son factores clave para futuros modelos de machine learning.\n",
    "* La limpieza de datos y el an√°lisis exploratorio proporcionaron una base s√≥lida para aplicaciones futuras, como la **predicci√≥n de si un Pok√©mon es legendario** o la **clasificaci√≥n de Pok√©mon seg√∫n sus estad√≠sticas**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comentarios acerca del ajuste Gaussiano**\n",
    "\n",
    "* El **ajuste Gaussiano** es una herramienta √∫til para determinar la distribuci√≥n de los datos y poder comparar las diferencias que existen con la estimaci√≥n inicial propuesta: utilizar la desviaci√≥n est√°ndar de la muestra para determinar qu√© regiones de la poblaci√≥n pueden ser consideradas como \"buenas\", \"malas\" o \"regulares\".\n",
    "* En este caso, el ajuste Gaussiano **no presenta diferencias significativas** con la estimaci√≥n inicial, por lo que se puede concluir que la distribuci√≥n de los datos es aproximadamente normal y que la discriminaci√≥n tabulada es v√°lida.\n",
    "* Este modelo ayuda a **determinar la desviaci√≥n est√°ndar de la poblaci√≥n**, lo que es √∫til para identificar la regi√≥n de \"buenos\" Pok√©mon, as√≠ como para futuros modelos de machine learning aplicados a las pr√≥ximas generaciones de Pok√©mon.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f13c03a9744b41b5af84d2b1f2af0a61",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> # Ap√©ndice\n",
    ">\n",
    "> ## **Comentarios**\n",
    "> * Cabe mencionar que aquellos cuadros de texto (Markdown) que est√°n subrayados como este en particular, son comentarios que se han realizado de car√°cter personal y humano para el desarrollo del c√≥digo y de esta tarea.\n",
    "> * Aquellos Markdown que no presenten el subrayado son los que se han generado mediante el uso de openAI.\n",
    "> * A su vez, aquellas celdas o textos escritos en ingl√©s son generados con o por openAI.\n",
    ">\n",
    "> Como, por ejemplo:\n",
    ">\n",
    " * The comments of openAI are in separate cells to facilitate reading and understanding of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8cf67536452d4b8990a20ded98188e1a",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
