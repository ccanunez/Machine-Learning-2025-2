{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ecddaed7eb5446599eb9374acef88d65",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> # **Tarea 1: Análisis Exploratorio de Datos - Pokémon**\n",
    "> #### **AST332-1 - 2025S2**\n",
    ">\n",
    "> Luciano Laroze, Cristián Núñez.\n",
    ">\n",
    "> *Universidad Técnica Federico Santa María, San Joaquín, Chile.*\n",
    ">\n",
    "> Nuestro repositorio de GitHub puede encontrarse en los siguientes usuarios\n",
    ">\n",
    "> C. Núñez [@ccanunez](https://github.com/ccanunez/Machine-Learning-2025-2.git, 'Reposotirio de C. Núñez')\n",
    ">\n",
    "> L. Laroze [@LuziVGC](https://github.com/LuziVGC, 'Reposotirio de L. Laroze')\n",
    ">\n",
    "> **Objetivo** : familiarizarse con el dataset, identificar patrones o anomalías y obtener una comprensión general de los datos para futuras aplicaciones de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b95987a17b5a45098fb77888fd5b4ced",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> # **Descripción y carga de datos**\n",
    ">\n",
    "> Se eligió el set de datos correspondiente al arreglo CSV de Pokémon.\n",
    ">\n",
    "> A continuación se cargan librerías para el uso (`numpy`) y manejo (`pandas`) de datos, así como la visualización (`matplotlib` y `seaborn`) de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global style settings for better quality plots\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 300,         # DPI for high-quality export\n",
    "    \"savefig.dpi\": 300,        # DPI when saving figures\n",
    "    \"figure.figsize\": (6, 4),  # A good balance: not too big, not too small\n",
    "    \"axes.labelsize\": 12,      # Axis label font size\n",
    "    \"axes.titlesize\": 14,      # Title font size\n",
    "    \"xtick.labelsize\": 10,     # X-tick label size\n",
    "    \"ytick.labelsize\": 10,     # Y-tick label size\n",
    "    \"lines.linewidth\": 1.5,    # Slightly thicker lines\n",
    "    \"lines.markersize\": 6,     # Marker size\n",
    "    \"legend.fontsize\": 10,     # Legend font size\n",
    "    \"grid.alpha\": 0.3,         # Softer grid\n",
    "    \"axes.grid\": True,         # Grid on by default\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos en formato DataFrame\n",
    "data = pd.read_csv('pokemon.csv')\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las primeras 5 líneas del DataFrame\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripción estadística del set de datos\n",
    "df = data.copy()\n",
    "describe = df.describe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "40db2221bb7f4607bb614fe19d7ee869",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "***The following cell was generated using openAI***\n",
    "\n",
    "> **Reason**: L. Laroze is a Pokémon Master (*someone who really knows how to play*) but C. Núñez is not. Actually, C. Núñez does not know anything at all of Pokémon. In order to understand the data without learning how to play we ask openAI to give us a complete summary of the different types of classes/columns that is presented. We also looked up the other Series/DataFrames like Disney+ and this kind of approach was similar to asking openAI what is the rating of metascore, imdb_rating, imdb_score, etc. Keeping in mind L. Laroze is more than a veteran in this matter of fact, we can modify the response of openAI if necessary given the talent of the Master.\n",
    "\n",
    "> **Prompt**: Given the following panda Data Frame of python output cell (*I gave the first 5 rows of the cell output and the data.describe() parameters*), tell me the range and description of the columns and rows. What is good and bad? How do I classify the scores? What's the min value and max value of each column?\n",
    "\n",
    "---\n",
    "\n",
    "Perfect — now we can rewrite the analysis using the **actual `data.describe()` output** you just provided.\n",
    "\n",
    "---\n",
    "\n",
    "# 📊 Pokémon DataFrame Summary\n",
    "\n",
    "Your dataset has **801 Pokémon entries (rows)** and at least **34 numeric columns** (stats, effectiveness multipliers, height, weight, etc.). Below is a breakdown of ranges, classification, and what “good” or “bad” means for each type of column.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Type Effectiveness (`against_*` columns)**\n",
    "\n",
    "* **Range:** `0.0 → 4.0`\n",
    "* **Meaning:** Multiplier applied to incoming damage of that type.\n",
    "\n",
    "  * `0` → **Immune** (best case).\n",
    "  * `0.25–0.5` → **Resistant** (takes reduced damage).\n",
    "  * `1` → **Neutral** (normal damage).\n",
    "  * `2–4` → **Weakness** (takes more damage, bad).\n",
    "* **Min/Max (from your data):**\n",
    "\n",
    "  * Min values: 0.0 (e.g. Normal immune to Ghost).\n",
    "  * Max values: 4.0 (quadruple weakness, e.g. Grass/Flying vs Ice).\n",
    "\n",
    "👉 **Good**: lower values (<1).\n",
    "👉 **Bad**: higher values (>1).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Base Stats**\n",
    "\n",
    "(Values used in Pokémon battles; higher = stronger)\n",
    "\n",
    "* **`hp` (Hit Points)**\n",
    "\n",
    "  * Range: `1 → 255`\n",
    "  * Good: ≥ 100 = bulky, survivable.\n",
    "  * Bad: ≤ 50 = frail.\n",
    "\n",
    "> * **`attack` (Attack)**\n",
    ">\n",
    ">  * Range: `5 → 185`\n",
    ">  * Good: ≥ 110 = strong physical attacker.\n",
    ">  * Bad: ≤ 50 = weak.\n",
    "\n",
    "> * **`defense` (Defense)**\n",
    ">\n",
    ">  * Range: `5 → 230`\n",
    ">  * Good: ≥ 110 = physical wall.\n",
    ">  * Bad: ≤ 50 = weak.\n",
    "\n",
    "* **`sp_attack` (Special Attack)**\n",
    "\n",
    "  * Range: `10 → 194`\n",
    "  * Good: ≥ 110 = strong special attacker.\n",
    "  * Bad: ≤ 50 = weak.\n",
    "\n",
    "* **`sp_defense` (Special Defense)**\n",
    "\n",
    "  * Range: `20 → 230`\n",
    "  * Good: ≥ 110 = special wall.\n",
    "  * Bad: ≤ 50 = fragile.\n",
    "\n",
    "* **`speed`**\n",
    "\n",
    "  * Range: `5 → 180`\n",
    "  * Good: ≥ 100 = very fast.\n",
    "  * Bad: ≤ 50 = very slow.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Physical Traits**\n",
    "\n",
    "* **`height_m`**\n",
    "\n",
    "  * Range: `0.1 → 14.5 m`\n",
    "  * Informational only; matters for flavor or some moves (e.g. Low Kick).\n",
    "\n",
    "* **`weight_kg`**\n",
    "\n",
    "  * Range: `0.1 → 999.9 kg`\n",
    "  * Heavier Pokémon benefit from moves like Heavy Slam but are vulnerable to Low Kick.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Demographics**\n",
    "\n",
    "* **`percentage_male`**\n",
    "\n",
    "  * Range: `0 → 100`\n",
    "  * 0 = all female/genderless, 100 = all male.\n",
    "  * Neutral — not good/bad, just species trait.\n",
    "\n",
    "* **`generation`**\n",
    "\n",
    "  * Range: `1 → 7`\n",
    "  * Indicates introduction era. Not about strength.\n",
    "\n",
    "* **`pokedex_number`**\n",
    "\n",
    "  * Range: `1 → 801`\n",
    "  * Pure identifier.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Legendary Status**\n",
    "\n",
    "* **`is_legendary`**\n",
    "\n",
    "  * Values: `0` (non-legendary), `1` (legendary).\n",
    "  * Legendary Pokémon generally have higher stats (base stat totals often >600).\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **Summary Table**\n",
    "\n",
    "| Column Group      | Range (min–max) | Good / Bad Meaning                      |\n",
    "| ----------------- | --------------- | --------------------------------------- |\n",
    "| `against_*`       | 0.0 → 4.0       | <1 = good (resist), >1 = bad (weakness) |\n",
    "| `hp`              | 1 → 255         | High = bulky, low = frail               |\n",
    "| `sp_attack`       | 10 → 194        | High = strong attacker                  |\n",
    "| `sp_defense`      | 20 → 230        | High = durable vs specials              |\n",
    "| `speed`           | 5 → 180         | High = moves first                      |\n",
    "| `height_m`        | 0.1 → 14.5      | Informational                           |\n",
    "| `weight_kg`       | 0.1 → 999.9     | Contextual                              |\n",
    "| `percentage_male` | 0 → 100         | Neutral descriptor                      |\n",
    "| `generation`      | 1 → 7           | Contextual                              |\n",
    "| `pokedex_number`  | 1 → 801         | Identifier only                         |\n",
    "| `is_legendary`    | 0 / 1           | 1 = Legendary Pokémon                   |\n",
    "\n",
    "---\n",
    "\n",
    "***This is the end of the cell generated using openAI***\n",
    ">\n",
    "> Some descriptions of the columns where not completely described by openAI, so we added them manually e.g., attack and defense stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b7ec13f3068346ba98dc65f4899dc3f0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> # **Limpieza de datos**\n",
    ">\n",
    "> Identificación de valores no definidos o faltantes\n",
    "> - Aquellos valores tipo NaN indican la etiqueta *Not A Number* por lo que es un valor que se desconoce o no está definido.\n",
    "> - Por medio de la operación **data.info()** es posible obtener información relevante y necesaria para identificar valores tipo NaN:\n",
    "> - Cabe mencionar que no todos los pokemones poseen un tipo 2, por lo que los valores NaN en esta columna no son un problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripción numérica del set de datos\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cfb4f585f6564a1b917a3f0bf8fde9e2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "|index |  parameter       |           number non-null |    type    |\n",
    "|----|------------------|-------------------------------|------------|\n",
    "| 27 |  height_m        |           781 non-null        |    float64 |\n",
    "| 31 |  percentage_male |    703 non-null               |    float64 |\n",
    "| 37 |  type2           |              417 non-null     |    object  |\n",
    "| 38 |  weight_kg       |          781 non-null         |    float64 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['height_m'], bins=50)\n",
    "plt.xlabel('height_m')\n",
    "plt.show()\n",
    "plt.hist(data['weight_kg'], bins=50)\n",
    "plt.xlabel('weight_kg')\n",
    "plt.show()\n",
    "plt.hist(data['percentage_male'], bins=50)\n",
    "plt.xlabel('percentage_male')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0fc515ee47604b75aee1f1e84e947458",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## **Explicación de valores NaN:**\n",
    ">\n",
    "> * height_m:\n",
    ">   * 20 valores NaN. \n",
    ">   * Debido a la presencia de outliers, los valores NaN serán reemplazados por la mediana de los datos\n",
    ">\n",
    "> * weight_kg:\n",
    ">   * 20 valores NaN. \n",
    ">   * Según la misma lógica, estos valores también serán reemplazados por la mediana de los datos\n",
    ">\n",
    "> * percentage_male: \n",
    ">   * 98 valores NaN.\n",
    ">   * No se reemplazaran porque no todos los pokémon tienen género\n",
    ">\n",
    "> * type2:\n",
    ">   * 384 valores NaN. \n",
    ">   * No es necesario reemplazarlos porque no todos los pokémon tienen segundo tipo\n",
    ">\n",
    "> En la siguiente línea reemplazamos los valores NaN mencionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['height_m'].fillna(data['height_m'].median(), inplace=True)\n",
    "data['weight_kg'].fillna(data['weight_kg'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e9956f8427874bf2a3dc8297075eacd6",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## **Filas o columnas duplicadas**\n",
    ">\n",
    "> Afortunadamente observamos que no contamos con filas ni columnas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e714cd0e679f4783b21b9c4a09d71985",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> # **Análisis estadístico**\n",
    ">\n",
    "> Para la visualización de datos según un Análisis Exploratorio de Datos (EDA) se utilizará la librería `seaborn` junto al apoyo de `matplotlib`. Según estas librerías contamos con el método `catplot` que permite visualizar datos categóricos y `displot` que permite visualizar datos numéricos. Además, se utilizará el método `plot` de `pandas` y el método `hist` de `matplotlib` para ayudar a comprender la relevancia y diferencia entre sets de datos. \n",
    ">\n",
    "> * Para algunos elementos nos daremos cuenta que es posible analizar y visualizar de manera similar o análoga el mismos set de datos empleando histograma u otro medio de visualización gráfica.\n",
    "> * El criterio de visulización se basa en la cantidad de datos que se desea visualizar y la forma en que se desea comprender los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "13c7433d4aff45a5b56bbc8c865899a0",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown",
    "execution_context_id": "60beab00-960c-4dff-adde-990e21a2fc8c",
    "execution_millis": 0,
    "execution_start": 1757466491356,
    "source_hash": "ec881b0a"
   },
   "source": [
    "> ## Análisis de variables categóricas\n",
    "> En esta sección se analizarán mediante distintos gráficos las variables categóricas del set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=data, x='is_legendary', kind='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fc0fb2d983a341c5a1213f539c891ef0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Observamos que la mayoría de los pokemones no son legendarios, por lo que se tiene una distribución desbalanceada de datos.\n",
    "> * Es posible representar estos datos como un histograma de dos bins o bien dentro de un gráfico de torta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for 1\n",
    "legendary = list(filter(lambda x : x // 1 == 1, list(data['is_legendary'])))\n",
    "print(f'el número de pokémon legendarios es {len(legendary)}')\n",
    "\n",
    "# Data for 0\n",
    "legendario = data['is_legendary']\n",
    "print(f'el número de pokémon normales es {len(list(legendario))-len(legendary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: 70 ones, 731 zeros -> two categories\n",
    "Leg = len(legendary)\n",
    "notLeg = len(list(legendario))-len(legendary)\n",
    "x = [notLeg, Leg]\n",
    "labels = [\"Normal\", \"Legendario\"]\n",
    "colors = plt.get_cmap(\"Blues\")(np.linspace(0.4, 0.8, len(x)))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(\n",
    "    x,\n",
    "    labels=labels,\n",
    "    colors=colors,\n",
    "    autopct=\"%1.1f%%\",             # show percentages\n",
    "    startangle=90,                 # start from the top\n",
    "    wedgeprops={\"linewidth\": 1, \"edgecolor\": \"white\"}\n",
    ")\n",
    "\n",
    "ax.set_title(\"Distribución de Clases\", fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6cf1cb0b8ce046e691658eca3b973329",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Con esto, podemos ver y discriminar de manera porcentual que la mayoría de los pokémon son normales y no legendarios. \n",
    ">\n",
    "> Veamos como se distribuyen estos pokémon según su tipo principal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3398607027fa42f797750519f9973b4f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> Podemos observar que la mayoría de los Pokémon legendarios pertenecen al tipo “Psíquico”. A pesar de esta tendencia en los datos, no es posible generar un modelo de aprendizaje automático que prediga si un Pokémon es legendario o no, debido a que la mayoría de los Pokémon no lo son. Sin embargo, un modelo de aprendizaje automático podría determinar que un Pokémon de tipo Lucha o Veneno no será legendario.\n",
    ">\n",
    "> En el gráfico hay otra caracterísitca que se puede observar, y es la poca cantidad de pokémon de tipo flying comparado al resto de tipos. Recordemos que los pokemon tienen el \"type1\" pero también pueden tener type2, veamos si en este gráfico podemos obtener información relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=data, x='type1', kind='count', height=8, aspect=1.5, hue = \"is_legendary\")\n",
    "plt.show()\n",
    "\n",
    "sns.catplot(data=data, x='type2', kind='count', height=8, aspect=1.5, hue = \"is_legendary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "50688e9c679e4a1fadae52444da99ca3",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Con este último plot se encontró que la mayoría de pokémon que presentan el tipo volador lo hacen en su caracterísitca de type2 en vez de type1. \n",
    "> * Esto se puede deber a un simple capricho de los desarrolladores del juego que condiciona a la hora de estudiar los tipos más recurrentes en los pokémon. \n",
    "> * Para ver cual es el tipo más usual, debemos hacer una suma directa de ambas características referentes a los tipos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipos = []\n",
    "cont = []\n",
    "for a in data[\"type1\"]:\n",
    "    if a not in tipos:\n",
    "        tipos.append(a)\n",
    "        cont.append(1)\n",
    "    else:\n",
    "        cont[tipos.index(a)] += 1\n",
    "tip2 = data[\"type2\"]\n",
    "for b in tip2:\n",
    "    if type(b) == str:\n",
    "        if b not in tipos:\n",
    "            tipos.append(b)\n",
    "            cont.append(1)\n",
    "        else:\n",
    "            cont[tipos.index(b)] += 1\n",
    "print(tipos)\n",
    "print(cont)\n",
    "plt.figure(figsize = (14,8))\n",
    "plt.bar(tipos, cont, color='blue')\n",
    "plt.xlabel('Tipos')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.title('Cantidad de Pokémon por Tipo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b7797ca1aa294b2cbb905b65d19e9403",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Con esto finalmente podemos ver que tipo de pokemon es más común, lo que puede ser relevante para un futuro modelo de machine learning utilizando varibles categóricas.\n",
    "\n",
    "> Otra pregunta que puede surgir es: ¿Hay pokémon que sean siempre machos o siempre hembra?\n",
    "> Para responder esto, analizaremos la variable percentage_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poke_sex(sex):\n",
    "    if sex < 50:\n",
    "        return 'primarly female'\n",
    "    elif sex > 50:\n",
    "        return 'primarly male'\n",
    "    else:\n",
    "        return 'no predilection'\n",
    "\n",
    "data['percentage_male'] = data.apply(lambda x: poke_sex(x['percentage_male']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=data, x='percentage_male', kind='count', height=8, aspect=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "71f064150c244a6eb869e1e2ed4d7641",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> Aquí podemos observar como la mayoría de los pokémon no tienen predilección de género, sin embargo, hay algunos pokémon que son mayormente machos o hembras. Creemos que esto debe ser por algo de carácter estadístico y no por azar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cc37525d59d34fdeb0a57a2b78b68ebe",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## Análisis de variables no categóricas\n",
    "> En esta sección se analizarán mediante histogramas y otras visualizaciones las variables no categóricas del set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hp'].plot(title='Vida base', kind='hist', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(data['hp'].mean(), 2) # calculo de la media de la vida base de los pokémon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6e3534006bdb41f0b3328404e7e156a1",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * En algunos casos es posible identificar outliers en los datos, por lo que se debe tener cuidado al momento de realizar análisis estadísticos.\n",
    "> * En este caso, se observa que la vida base de los pokémon se distribuye de manera similar a una distribución normal, con una media de 69.3 puntos de vida base.\n",
    "> * Es más fácil identificar outliers dentro de un digrama de caja y bigote pues se puede identificar el rango intercuartílico y los valores atípicos.\n",
    "> * Vamos a realizar diagramas de caja y bigote para las estadísticas base de los pokémon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['hp'].mean().round(2), data['hp'].median())\n",
    "data['hp'].plot(title='Vida Inicial', kind='box')\n",
    "plt.hlines(y=data['hp'].mean(), xmin=0, xmax=2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['hp'].plot(title='Vida Inicial', kind='box')\n",
    "data['hp'].plot(title='Vida Inicial', kind='box', ylim=(5,130))\n",
    "plt.hlines(y=data['hp'].mean(), xmin=0, xmax=2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['attack'].plot(title='Ataque base', kind='box')\n",
    "plt.hlines(y=data['attack'].mean(), xmin=0, xmax=2, color='red')\n",
    "#data['hp'].plot(title='Vida Inicial', kind='box', ylim=(5,130))\n",
    "plt.show()\n",
    "print(\"el promedio de ataque es\", round(data['attack'].mean(), 2), \"y la mediana es\", data['attack'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['defense'].plot(title='Defensa base', kind='box')\n",
    "plt.hlines(y=data['defense'].mean(), xmin=0, xmax=2, color='red')\n",
    "#data['hp'].plot(title='Defensa Base', kind='box', ylim=(5,130))\n",
    "plt.show()\n",
    "print(\"el promedio de defensa es\", data['defense'].mean().round(2), \"y la mediana es\", data['defense'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sp_attack'].plot(title='Ataque especial base', kind='box')\n",
    "plt.hlines(y=data['sp_attack'].mean(), xmin=0, xmax=2, color='red')\n",
    "#data['hp'].plot(title='Vida Inicial', kind='box', ylim=(5,130))\n",
    "plt.show()\n",
    "print(\"el promedio de ataque especial es\", data['sp_attack'].mean().round(2), \"y la mediana es\", data['sp_attack'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sp_defense'].plot(title='Defensa especial base', kind='box')\n",
    "plt.hlines(y=data['sp_defense'].mean(), xmin=0, xmax=2, color='red')\n",
    "#data['hp'].plot(title='Vida Inicial', kind='box', ylim=(5,130))\n",
    "plt.show()\n",
    "print(\"el promedio de defensa especial es\", data['sp_defense'].mean().round(2), \"y la mediana es\", data['sp_defense'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speed'].plot(title='Velocidad base', kind='box')\n",
    "plt.hlines(y=data['speed'].mean(), xmin=0, xmax=2, color='red')\n",
    "#data['hp'].plot(title='Vida Inicial', kind='box', ylim=(5,130))\n",
    "plt.show()\n",
    "print(\"el promedio de velocidades es\", data['speed'].mean().round(2), \"y la mediana es\", data['speed'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "025907071b8c45c2aa710c6b67b70c09",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * En los diagramas anteriores podemos notar como todas las estadísitcas tienen outliers presentes.\n",
    "> * Estos outliers generalmente se encuentran en las estadísticas más altas, lo que se ve reflejado en la diferencia entre la mediana y el promedio de los datos, donde la mediana siempre es menor que el promedio.\n",
    "> * Este comportamiento implica que si hubiesemos teníamos datos faltantes los deberíamos reemplazar por la mediana de los datos y no por la media.\n",
    "> * Por último, podemos notar que el comportamiento estatístico de las diferentes estadísticas es similar, y para observar esto mejor, hacemos un gráfico de densidad de probabilidad de estas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['attack'].plot(kind='density', label='Ataque')\n",
    "data['defense'].plot(kind='density', label='Defensa')\n",
    "data['sp_attack'].plot(kind='density', label='Ataque especial')\n",
    "data['sp_defense'].plot(kind='density', label='Defensa especial')\n",
    "data['speed'].plot(kind='density', label='Velocidad')\n",
    "plt.legend(title='Estadísticas')\n",
    "plt.xlim(0,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_mean = data['attack'].mean()\n",
    "attack_std = data['attack'].std()\n",
    "data['attack'].plot(kind='density')\n",
    "plt.axvspan(0, attack_mean-attack_std, alpha=0.3, color='red', label='Bad')\n",
    "plt.axvspan(attack_mean-attack_std, attack_mean+attack_std, alpha=0.3, color='orange', label='Regular')\n",
    "plt.axvspan(attack_mean+attack_std, 200, alpha=0.3, color='green', label='Good')\n",
    "plt.xlim(0,200)\n",
    "plt.legend(title='Performace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defense_mean = data['defense'].mean()\n",
    "defense_std = data['defense'].std()\n",
    "data['defense'].plot(kind='density')\n",
    "plt.axvspan(0, defense_mean-defense_std, alpha=0.3, color='red', label='Bad')\n",
    "plt.axvspan(defense_mean-defense_std, defense_mean+defense_std, alpha=0.3, color='orange', label='Regular')\n",
    "plt.axvspan(defense_mean+defense_std, 230, alpha=0.3, color='green', label='Good')\n",
    "plt.xlim(0,230)\n",
    "plt.legend(title='Performace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_attack_mean = data['sp_attack'].mean()\n",
    "sp_attack_std = data['sp_attack'].std()\n",
    "data['sp_attack'].plot(kind='density')\n",
    "plt.axvspan(0, sp_attack_mean-sp_attack_std, alpha=0.3, color='red', label='Bad')\n",
    "plt.axvspan(sp_attack_mean-sp_attack_std, sp_attack_mean+sp_attack_std, alpha=0.3, color='orange', label='Regular')\n",
    "plt.axvspan(sp_attack_mean+sp_attack_std, 210, alpha=0.3, color='green', label='Good')\n",
    "plt.xlim(0,210)\n",
    "plt.legend(title='Performace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_defense_mean = data['sp_defense'].mean()\n",
    "sp_defense_std = data['sp_defense'].std()\n",
    "data['sp_defense'].plot(kind='density')\n",
    "plt.axvspan(0, sp_defense_mean-sp_defense_std, alpha=0.3, color='red', label='Bad')\n",
    "plt.axvspan(sp_defense_mean-sp_defense_std, sp_defense_mean+sp_defense_std, alpha=0.3, color='orange', label='Regular')\n",
    "plt.axvspan(sp_defense_mean+sp_defense_std, 230, alpha=0.3, color='green', label='Good')\n",
    "plt.xlim(0,230)\n",
    "plt.legend(title='Performace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_mean = data['speed'].mean()\n",
    "speed_std = data['speed'].std()\n",
    "data['speed'].plot(kind='density')\n",
    "plt.axvspan(0, speed_mean-speed_std, alpha=0.3, color='red', label='Bad')\n",
    "plt.axvspan(speed_mean-speed_std, speed_mean+speed_std, alpha=0.3, color='orange', label='Regular')\n",
    "plt.axvspan(speed_mean+speed_std, 200, alpha=0.3, color='green', label='Good')\n",
    "plt.xlim(0,200)\n",
    "plt.legend(title='Performace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1b83ae5e3a834cfba85d1af16f76e084",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Es importante mencionar que no se realizó un ajuste de curva exhaustivo para determinar la distribución de los datos, por lo que se debe tener cuidado al interpretar los resultados.\n",
    "> * Para mejorar el rango estadístico el cual se considera \"bueno\", \"malo\" o \"regular\" sería factible poder realizar un ajuste de curva Voight, Lorenztiano o bien Gausseano para determinar una desviación estándar real de los datos.\n",
    "> * El criterio empleado en este gráfico fue el uso de la desviación estándar de los datos, donde se considera que los datos que se encuentran a una desviación estándar de la media son \"regulares\", los que se encuentran a dos desviaciones estándar de la media son \"buenos\" y los que se encuentran a más de dos desviaciones estándar de la media son \"malos\".\n",
    ">\n",
    "> ## **Ajuste de curva Gausseano**\n",
    ">\n",
    "> *This code was generated using openAI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "\n",
    "s = data['speed'].dropna().to_numpy()\n",
    "\n",
    "# Fit Gaussian (mean, std)\n",
    "mu, sigma = norm.fit(s)\n",
    "\n",
    "# Build density curves\n",
    "kde = gaussian_kde(s)\n",
    "x = np.linspace(0, 200, 1000)\n",
    "\n",
    "# --- Plot ---\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# KDE (empirical density)\n",
    "ax.plot(x, kde(x), lw=2, label='KDE Density')\n",
    "\n",
    "# Gaussian PDF (fit)\n",
    "ax.plot(x, norm.pdf(x, mu, sigma), 'r--', lw=2, label=f'Gaussian Fit (μ={mu:.1f}, σ={sigma:.1f})')\n",
    "\n",
    "# Shaded regions (same as before)\n",
    "ax.axvspan(0, mu - sigma, alpha=0.3, color='red', label='Bad')\n",
    "ax.axvspan(mu - sigma, mu + sigma, alpha=0.3, color='orange', label='Regular')\n",
    "ax.axvspan(mu + sigma, 200, alpha=0.3, color='green', label='Good')\n",
    "\n",
    "# Vertical lines for ±1σ and ±2σ\n",
    "for k, style in zip([1, 2], ['--', ':']):\n",
    "    ax.axvline(mu - k*sigma, color='black', linestyle=style, alpha=0.7)\n",
    "    ax.axvline(mu + k*sigma, color='black', linestyle=style, alpha=0.7)\n",
    "\n",
    "ax.set_xlim(0, 200)\n",
    "ax.set_xlabel('Speed')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Speed Distribution with Gaussian Fit')\n",
    "ax.legend(title='Performance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e906c4968f7b4a5d83674e4eb7fa970b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This following cell was generated using openAI*\n",
    "\n",
    "**Reason**: We had a problem understanding why the std of the fit was the same as the std of the data. We asked openAI to explain why this was happening.\n",
    "\n",
    "Good catch 👌 — this happens because of **what you mean by “fit.”**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 Why they match\n",
    "\n",
    "When you do this:\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "mu, sigma = norm.fit(data)\n",
    "```\n",
    "\n",
    "* `norm.fit` uses **Maximum Likelihood Estimation (MLE)** to estimate the Gaussian parameters.\n",
    "* For a *true normal distribution*, the MLE for the mean ($\\mu$) is the **sample mean**, and the MLE for the standard deviation ($\\sigma$) is **almost** the sample standard deviation.\n",
    "\n",
    "So:\n",
    "\n",
    "* **Mean**: exactly equal to the sample mean.\n",
    "* **Standard deviation**:\n",
    "\n",
    "  * The usual *pandas* `data.std()` uses the **unbiased estimator** (divides by $n-1$).\n",
    "  * `norm.fit` (MLE) uses the **biased estimator** (divides by $n$).\n",
    "\n",
    "For large samples, the difference is tiny, so they look “the same.”\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ The math\n",
    "\n",
    "* Sample mean:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n x_i\n",
    "  $$\n",
    "\n",
    "* Sample variance (unbiased, what `pandas.std()` does):\n",
    "\n",
    "  $$\n",
    "  s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\hat{\\mu})^2\n",
    "  $$\n",
    "\n",
    "* MLE variance (used in `norm.fit`):\n",
    "\n",
    "  $$\n",
    "  \\sigma^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\hat{\\mu})^2\n",
    "  $$\n",
    "\n",
    "The difference is only the denominator ($n$ vs $n-1$).\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ How to check in Python\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "s = data['speed'].dropna().to_numpy()\n",
    "\n",
    "# Pandas/NumPy std (unbiased, denominator = n-1)\n",
    "print(\"Sample std (unbiased):\", np.std(s, ddof=1))\n",
    "\n",
    "# Population std (biased, denominator = n)\n",
    "print(\"Sample std (MLE form):\", np.std(s, ddof=0))\n",
    "\n",
    "# Fit with scipy\n",
    "mu_fit, sigma_fit = norm.fit(s)\n",
    "print(\"Sigma from norm.fit:\", sigma_fit)\n",
    "```\n",
    "\n",
    "You’ll see:\n",
    "\n",
    "* `np.std(s, ddof=0)` ≈ `sigma_fit`\n",
    "* `np.std(s, ddof=1)` is slightly larger\n",
    "\n",
    "---\n",
    "\n",
    "✅ **So in short:**\n",
    "The Gaussian “fit” and the raw data std match because the MLE estimate of σ is basically the same as the dataset’s standard deviation (with denominator $n$). The fit isn’t doing anything fancier — it just reproduces the sample statistics.\n",
    "\n",
    "---\n",
    "\n",
    "👉 Do you want me to also explain what happens if your data **isn’t Gaussian** (skewed or heavy-tailed) — i.e. how the “fit σ” can then differ significantly from the raw sample std?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "s = data['speed'].dropna().to_numpy()\n",
    "\n",
    "# Pandas/NumPy std (unbiased, denominator = n-1)\n",
    "print(\"Sample std (unbiased):\", np.std(s, ddof=1))\n",
    "\n",
    "# Population std (biased, denominator = n)\n",
    "print(\"Sample std (MLE form):\", np.std(s, ddof=0))\n",
    "\n",
    "# Fit with scipy\n",
    "mu_fit, sigma_fit = norm.fit(s)\n",
    "print(\"Sigma from norm.fit:\", sigma_fit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9424abdd635242939e661e0e4244018c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## **Comentarios acerca del ajuste Gaussiano**\n",
    ">\n",
    "> * El ajuste Gaussiano es una herramienta útil para determinar la distribución de los datos y poder comparar la(s) diferencias que existe con la estimación inicial propuesta: utilizar la desviación estándar de la muestra para determinar qué regiones de la población pueden ser consideradas como \"buenas\", \"malas\" o \"regulares\".\n",
    "> * En este caso, el ajuste Gaussiano no presenta diferencias significativas con la estimación inicial propuesta, por lo que se puede concluir que la distribución de los datos es aproximadamente normal y que la discriminación tabulada es válida.\n",
    "> * Este modelo ayuda a determinar la desviación estándar de la población, lo que es útil para determinar la región de \"buenos\" pokémon, así como futuros modelos de machine learning para las próximas generaciones de pokémon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "43eae1963d8f4731a2c9b1dfedb7beee",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## **Análisis de la suma total de atributos**\n",
    ">\n",
    "> * Realizaremos un análisis de la suma total de todos los atributos de los pokémon para ver si es posible identificar algún patrón.\n",
    "> * Para esto, se sumarán los atributos de cada pokémon y se graficará la suma total de los atributos vs la vida base de los pokémon.\n",
    "> * Se espera que los pokémon legendarios tengan una suma total de atributos mayor a los pokémon no legendarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df['stats'] = df['hp'] + df['attack'] + df['defense'] + df['sp_attack'] + df['sp_defense'] + df['speed']\n",
    "\n",
    "plt.xlabel('Estadísticas')\n",
    "plt.ylabel('Vida')\n",
    "plt.title('Vida vs Estadísticas')\n",
    "\n",
    "c1 = True\n",
    "c2 = True\n",
    "for i in range(len(df)):\n",
    "    if df['is_legendary'][i] == 1:\n",
    "        if c1 == True:\n",
    "            plt.scatter(df['stats'][i], df['hp'][i], color='red', label = \"Legendario\")\n",
    "            c1 = False\n",
    "        plt.scatter(df['stats'][i], df['hp'][i], color='red')\n",
    "    else:\n",
    "        if c2 == True:\n",
    "            plt.scatter(df['stats'][i], df['hp'][i], color='blue', label='No legendario')\n",
    "            c2 = False\n",
    "        plt.scatter(df['stats'][i], df['hp'][i], color='blue')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ebb35be073274dbfa5b8e222c71d0f04",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Notamos claramente que hay  una tendencia de que los pokémon legendarios tengan una suma total de atributos mayor a los pokémon no legendarios, con unos pocos outliers legendarios. Para una visualización más atractiva, hacemos un plot de densidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "16b29f8a13d54e9cbb9e069102ff52ee",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ### Visualización de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "48dbb404a6cf490b94efddb94b3a91ae",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This code was generated using openAI*\n",
    "\n",
    "> **Reason**: The code was generated using openAI because it was not possible to generate the code without the use of openAI. The code was generated using the following prompt: \"Generate a code that plots a kdeplot of the stats column of the df dataframe, using the is_legendary column as the hue. The x axis should be limited to the min and max values of the stats column. The plot should be a FacetGrid with an aspect of 5. The plot should be shown using plt.show()\"\n",
    "\n",
    "> * Es crucial considerar que un gráfico de densidad es una herramienta efectiva para visualizar la distribución de los datos, ya que muestra la función de densidad de probabilidad normalizada i.e., la integral de dicha función equivalente a la unidad. A diferencia de un gráfico de dispersión no se muestran los puntos en particular, sino una tendencia general de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Correcting the error by using the correct column name 'stats' in the FacetGrid map function\n",
    "as_fig = sns.FacetGrid(df, hue='is_legendary', aspect=5)\n",
    "as_fig.map(sns.kdeplot, 'stats', fill=True)\n",
    "\n",
    "max_value = df['stats'].max()\n",
    "min_value = df['stats'].min()\n",
    "as_fig.set(xlim=(min_value, max_value))\n",
    "as_fig.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "76efa2c54a044465a492ec1bba802d31",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> * Con las funciones de densidad de probabilidad se puede ver de una mejor manera la tendencia de los legendarios a tener estadísticas altas, mientras que los no legendarios presentan una distribución más uniforme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b66961d321e04af88e468534bd6ffd65",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ### Análisis de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fix the error, we need to provide another Series to calculate the correlation with.\n",
    "# For example, if we want to calculate the correlation between 'stats' and 'hp':\n",
    "\n",
    "correlation = df['stats'].corr(df['hp'])\n",
    "print(\"Correlation between stats and hp:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.copy()[['attack', 'base_egg_steps', 'defense', 'experience_growth', 'hp', 'percentage_male', 'pokedex_number', 'sp_attack', 'sp_defense', 'speed', 'generation', 'is_legendary', 'stats']]\n",
    "print(corr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0cd829b0f36a416a9379ebcf76a59fab",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This code was generated using openAI*\n",
    "\n",
    "> **Reason**: The code was generated using openAI because it was not possible to generate the code without the use of openAI. The code was generated using the following prompt: \"Generate a code that plots a heatmap of the correlation matrix of the corr_df dataframe. The heatmap should have a size of 15x12, the annotations should be formatted to 2 decimal places, the colormap should be 'coolwarm' and the center of the colormap should be 0. The title of the heatmap should be 'Correlation Matrix' with a font size of 14. The plot should be shown using plt.show()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fix the error, we need to exclude non-numeric columns from the DataFrame before calculating the correlation matrix.\n",
    "\n",
    "# Select only numeric columns from the DataFrame\n",
    "numeric_df = corr_df.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate the correlation matrix for the numeric columns\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Matrix\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4507dc96357340c79d4caa2b75c9faa9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This following prompt was generated using openAI*\n",
    "\n",
    "Yes — there are several good options depending on how you want to *calculate* and how you want to *visualize* the correlation matrix. Let’s go step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Calculate correlation matrix**\n",
    "\n",
    "The standard way is with **pandas**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "corr = df.corr()   # by default: Pearson correlation\n",
    "```\n",
    "\n",
    "You now have a DataFrame with the pairwise correlations.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Visualize with Seaborn**\n",
    "\n",
    "Seaborn doesn’t calculate correlations itself, but it’s *great for plotting* them.\n",
    "Most common tool: `sns.heatmap`\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Matrix\", fontsize=14)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "* `annot=True` → show values inside the cells.\n",
    "* `center=0` → colors diverge around zero.\n",
    "* `cmap=\"coolwarm\"` → blue = negative, red = positive.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Alternative visualization libraries**\n",
    "\n",
    "* **`seaborn.clustermap`**\n",
    "  Automatically clusters correlated variables for structure discovery:\n",
    "\n",
    "  ```python\n",
    "  sns.clustermap(corr, annot=True, cmap=\"coolwarm\", center=0)\n",
    "  ```\n",
    "* **`matplotlib` (manual)** → use `plt.matshow` or `ax.imshow`.\n",
    "* **`pingouin`** (a stats library) → has `pingouin.pairwise_corr` for more advanced correlations.\n",
    "* **`plotly`** → interactive heatmaps if you want hover info:\n",
    "\n",
    "  ```python\n",
    "  import plotly.express as px\n",
    "  fig = px.imshow(corr, text_auto=True, color_continuous_scale=\"RdBu_r\", zmin=-1, zmax=1)\n",
    "  fig.show()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Best practice\n",
    "\n",
    "* **pandas** for computing (`.corr()`).\n",
    "* **seaborn.heatmap** for a clean, static plot.\n",
    "* **plotly** if you need interactive exploration.\n",
    "\n",
    "---\n",
    "\n",
    "Good — let’s interpret that number carefully.\n",
    "\n",
    "A correlation coefficient (usually Pearson’s *r*) ranges from **–1 to +1**:\n",
    "\n",
    "* **+1** → perfect positive linear relationship (as X goes up, Y goes up in lockstep).\n",
    "* **0** → no linear relationship.\n",
    "* **–1** → perfect negative linear relationship (as X goes up, Y goes down in lockstep).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 Your case: **r = 0.6206**\n",
    "\n",
    "* **Direction**: Positive → as one variable increases, the other tends to increase.\n",
    "* **Strength**: 0.62 is generally interpreted as a **moderately strong** correlation.\n",
    "\n",
    "  * Rough scale (rule of thumb, varies by field):\n",
    "\n",
    "    * 0.0–0.2 → negligible\n",
    "    * 0.2–0.4 → weak\n",
    "    * 0.4–0.7 → moderate\n",
    "    * 0.7–0.9 → strong\n",
    "    * 0.9–1.0 → very strong / almost deterministic\n",
    "\n",
    "So: **a moderately strong positive linear relationship**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 What it implies\n",
    "\n",
    "* The two variables share a noticeable trend, but it’s **not perfect**.\n",
    "* You could use one to *partially* predict the other, but there’s still a lot of variance unexplained.\n",
    "\n",
    "Formally, the **coefficient of determination** (*r²*) = (0.6206)² ≈ **0.385**.\n",
    "That means about **38.5% of the variance** in one variable can be explained by a linear relationship with the other. The remaining \\~61.5% is noise or other factors.\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ **Caution**:\n",
    "\n",
    "* Correlation ≠ causation.\n",
    "* Nonlinear relationships might not show up well in Pearson’s *r*.\n",
    "* Outliers can inflate or deflate the value.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "55325f0d2f704e078758f395ed73b9a2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## **Comentarios sobre la matriz de correlación obtenida**\n",
    ">\n",
    "> **Correlación entre vida base y suma total de atributos**\n",
    ">\n",
    "> * Es posible observar que la correlación entre la vida base y la suma total de atributos es de 0.62, lo que indica que existe una relación lineal moderada entre estas dos variables.\n",
    "> * Esto se debe a que los pokémon legendarios tienen una suma total de atributos mayor a los pokémon no legendarios.\n",
    "> * Por lo tanto, es posible que los pokémon legendarios tengan una vida base mayor a los no legendarios.\n",
    ">\n",
    "> **Correlación entre pokedex_number y generación**\n",
    ">\n",
    "> * Es posible observar que la correlación entre el pokedex_number y la generación es de 0.99, lo que indica que existe una relación lineal cuasi-perfecta entre estas dos variables.\n",
    "> * Esto se debe a que los pokémon se ordenan por generación y por número de pokedex, donde los pokémon de la generación 1 corresponden a números de 1 a 151, los de generación 2 de 152 a 251 y así aumentando, por lo que es lógico que exista una relación lineal entre estas dos variables. \n",
    "> * No es de exactamente 1.00 porque el número de pokémon es diferente en cada generación.\n",
    ">\n",
    "> **Correlación entre is_legendary y stats**\n",
    ">\n",
    "> * Se observa que la correlación entre estas variables es de 0.49, lo que indica que existe una relación lineal de 0.25 i.e., 25% de tendencia lineal entre estas dos variables.\n",
    "> * Este bajo porcentaje no proporciona información de la tendencia de encontrar pokémon legendarios con altas estadísticas (como si lo hace el plot de densidad), sino que la correlación entre las variables no es estrictamente lineal.\n",
    "> * Para realizar un estudio específico entre la tendencia de encontrar pokémon legendarios con altas estadísticas se debe realizar un análisis de regresión lineal o bien un análisis de regresión logística.\n",
    ">\n",
    "> **Correlación entre is_legendary y base_egg_steps**\n",
    ">\n",
    "> * Se observa una correlación sorprendentemente alta entre las variables base_egg_steps e is_legendary. \n",
    "> * Para entenderla, debemos saber que base_egg_steps esta relacionado con la crianza que el jugador puede hacer en los juegos de pokémon, donde recibes huevos de los cuales nacerán estos. Entonces la correlación alta implica que se requiere de una mayor cantidad de pasos para que eclosionen los huevos lo que mantiene el status de pokémon legendarios.\n",
    ">\n",
    "> **Correlación de percentage_male**\n",
    ">\n",
    "> * Gracias a la matriz de correlación podemos fijarnos en que valores pueden afectar a la probabilidad de que un pokémon sea macho o hembra. Notemos que en realidad, no hay ningún dato que se correlacione directamente con esta probabilidad. La única que sobresale un poco es la relación entre attack y percentage_male, lo que podría llegar a decir que los pokémon más ofensivos suelen ser machos, pero no la correlación no es lo suficientemente fuerte como para concluir algo relevante.\n",
    ">\n",
    "> **Correlación de defense y sp_defense**\n",
    ">\n",
    "> * En general podemos atribuir una relación entre estas dos variables, lo que podría llegar a decir que los pokémon que tienen una defensa física alta suelen tener una defensa especial alta i.e., son buenos defensores en general.\n",
    "> * Esto podría llegar a ser relevante para un futuro modelo de machine learning.\n",
    ">\n",
    "> **Correlación entre attack y sp_attack**\n",
    ">\n",
    "> * A diferencia de la defensa, no se puede concluir o relacionar linealmente el ataque físico con el ataque especial. No es esperable que si un pokémon tenga un ataque físico alto, tenga a su vez un ataque especial alto.\n",
    "> * Esto podría llegar a ser relevante para un futuro modelo de machine learning.\n",
    ">\n",
    "> **Correlación entre características de ataque y defensa en relación a los stats totales**\n",
    ">\n",
    "> * En general observamos una tendencia trivial respecto a la linealidad entre los parámetros que componen los stats y los stats totales. Esto es esperable, ya que los stats totales son la suma de los parámetros que los componen.\n",
    "\n",
    "*The following english cells were generated using openAI*\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 1. Linear Regression\n",
    "\n",
    "* **Goal**: predict a **continuous value**.\n",
    "  Example: predicting **height** in cm from age, or the **price of a house** from its square meters.\n",
    "* **Equation**:\n",
    "\n",
    "  $$\n",
    "  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\varepsilon\n",
    "  $$\n",
    "\n",
    "  where $y$ is a continuous variable (real numbers).\n",
    "* **Assumptions**:\n",
    "\n",
    "  * Linear relationship between predictors and the dependent variable.\n",
    "  * Residuals are normally distributed with constant variance (homoscedasticity).\n",
    "* **Output**: a real number (can be negative, fractional, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 2. Logistic Regression\n",
    "\n",
    "* **Goal**: predict a **probability** of belonging to a class, usually binary (0 or 1).\n",
    "  Example: Will a student pass (yes/no)? Is an email spam (yes/no)?\n",
    "* **Equation** (uses the sigmoid function to constrain values between 0–1):\n",
    "\n",
    "  $$\n",
    "  P(y=1 \\mid x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots)}}\n",
    "  $$\n",
    "* **Assumptions**:\n",
    "\n",
    "  * The dependent variable is categorical (often binary).\n",
    "  * Predictors have a linear relationship with the logit (log-odds).\n",
    "* **Output**:\n",
    "\n",
    "  * A **probability between 0 and 1**.\n",
    "  * If you apply a threshold (e.g., 0.5), it becomes a classification (0 or 1).\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ Key Differences\n",
    "\n",
    "| Aspect             | Linear Regression                        | Logistic Regression               |\n",
    "| ------------------ | ---------------------------------------- | --------------------------------- |\n",
    "| Dependent variable | Continuous                               | Categorical (binary, multinomial) |\n",
    "| Output             | Real number                              | Probability (0–1)                 |\n",
    "| Model function     | Linear (straight line)                   | Sigmoid / logit                   |\n",
    "| Estimation method  | Least squares (MSE)                      | Maximum likelihood                |\n",
    "| Typical use        | Predicting values (prices, measurements) | Classification (yes/no, 0/1)      |\n",
    "\n",
    "---\n",
    "\n",
    "👉 Short version:\n",
    "\n",
    "* If your **Y is continuous numeric** → use **linear regression**.\n",
    "* If your **Y is categorical (0/1)** → use **logistic regression**.\n",
    "\n",
    "---\n",
    "\n",
    "> * Por otro lado, es suficiente realizar un gráfico de densidad de probabilidad para visualizar la distribución de los datos y la relación entre las variables tal y como se mostró anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ab04b8d878c14ba98715e5f083d01cda",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> ## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7a19a6d1f2e34be686149ddc27e0a3e4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This code was generated using openAI*\n",
    "\n",
    ">**Reason**: Esta parte del código fue mejorada utilizando ChatGPT-5 debido a la mejora de visualización de los parámetros importantes y relevantes escritos de manera ordenada y clara.\n",
    "\n",
    "---\n",
    "\n",
    "Durante este análisis exploratorio de datos (EDA) del set de datos de Pokémon, se realizaron diversas tareas para comprender mejor las características y patrones presentes en los datos. A continuación, se resumen los hallazgos más relevantes:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Descripción y limpieza de datos**\n",
    "\n",
    "* El dataset contiene **801 entradas y 41 columnas**, incluyendo estadísticas base, tipos de Pokémon y características demográficas.\n",
    "* Se identificaron valores faltantes en las columnas `height_m`, `weight_kg`, `percentage_male` y `type2`.\n",
    "\n",
    "  * Los valores faltantes en `height_m` y `weight_kg` fueron reemplazados por la **mediana**.\n",
    "  * Los de `percentage_male` y `type2` no fueron modificados debido a su naturaleza inherente.\n",
    "* No se encontraron filas duplicadas en el dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Análisis de variables categóricas**\n",
    "\n",
    "* La mayoría de los Pokémon **no son legendarios** (731 normales vs. 70 legendarios).\n",
    "* Los tipos más comunes son `water`, `normal` y `grass`, mientras que el tipo `flying` es más frecuente como tipo secundario (`type2`).\n",
    "* La distribución de género mostró que la mayoría de los Pokémon no tienen predilección de género, aunque algunos son predominantemente machos o hembras.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Análisis de variables numéricas**\n",
    "\n",
    "* Las estadísticas base (`hp`, `attack`, `defense`, `sp_attack`, `sp_defense`, `speed`) presentan **distribuciones similares**, con *outliers* en los valores altos.\n",
    "* Se utilizó la **desviación estándar** para clasificar las estadísticas como \"buenas\", \"regulares\" o \"malas\".\n",
    "* Un **ajuste Gaussiano** confirmó que las distribuciones de las estadísticas son aproximadamente normales.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Análisis de correlación**\n",
    "\n",
    "* Se encontró una correlación **moderada (0.62)** entre la vida base (`hp`) y la suma total de estadísticas (`stats`).\n",
    "* La correlación entre el número de Pokédex y la generación es **casi perfecta (0.99)**, lo cual es esperable debido al orden en que se introducen los Pokémon.\n",
    "* Los Pokémon legendarios tienden a tener **estadísticas totales más altas**, pero la correlación entre `is_legendary` y `stats` es **moderada (0.49)**, indicando que no todos los Pokémon con estadísticas altas son legendarios.\n",
    "* Se observó una **correlación alta** entre `base_egg_steps` e `is_legendary`, lo que sugiere que los Pokémon legendarios requieren más pasos para eclosionar.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Visualización de datos**\n",
    "\n",
    "* Se utilizaron gráficos de densidad, diagramas de caja y bigote, gráficos de barras y mapas de calor para visualizar las distribuciones y relaciones entre las variables.\n",
    "* Los gráficos de densidad mostraron claramente la tendencia de los Pokémon legendarios a tener estadísticas más altas.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Conclusiones generales**\n",
    "\n",
    "* El dataset está bien estructurado y permite identificar patrones claros, como la relación entre las estadísticas y el estatus legendario.\n",
    "* Las **estadísticas base y los tipos de Pokémon** son factores clave para futuros modelos de machine learning.\n",
    "* La limpieza de datos y el análisis exploratorio proporcionaron una base sólida para aplicaciones futuras, como la **predicción de si un Pokémon es legendario** o la **clasificación de Pokémon según sus estadísticas**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comentarios acerca del ajuste Gaussiano**\n",
    "\n",
    "* El **ajuste Gaussiano** es una herramienta útil para determinar la distribución de los datos y poder comparar las diferencias que existen con la estimación inicial propuesta: utilizar la desviación estándar de la muestra para determinar qué regiones de la población pueden ser consideradas como \"buenas\", \"malas\" o \"regulares\".\n",
    "* En este caso, el ajuste Gaussiano **no presenta diferencias significativas** con la estimación inicial, por lo que se puede concluir que la distribución de los datos es aproximadamente normal y que la discriminación tabulada es válida.\n",
    "* Este modelo ayuda a **determinar la desviación estándar de la población**, lo que es útil para identificar la región de \"buenos\" Pokémon, así como para futuros modelos de machine learning aplicados a las próximas generaciones de Pokémon.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f13c03a9744b41b5af84d2b1f2af0a61",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "> # Apéndice\n",
    ">\n",
    "> ## **Comentarios**\n",
    "> * Cabe mencionar que aquellos cuadros de texto (Markdown) que están subrayados como este en particular, son comentarios que se han realizado de carácter personal y humano para el desarrollo del código y de esta tarea.\n",
    "> * Aquellos Markdown que no presenten el subrayado son los que se han generado mediante el uso de openAI.\n",
    "> * A su vez, aquellas celdas o textos escritos en inglés son generados con o por openAI.\n",
    ">\n",
    "> Como, por ejemplo:\n",
    ">\n",
    " * The comments of openAI are in separate cells to facilitate reading and understanding of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8cf67536452d4b8990a20ded98188e1a",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
